year,title,abstract,winner,university,degree
2003,Adaptive Motion for Quadruped Robots,"Robotic soccer is a complex task that requires multiple autonomous agents to collaborate in an adversarial environment to achieve specific objectives. The Sony quadruped robots are fully autonomous with onboard computation, visual perception, and sophisticated head, leg, and tail motions. The robotic soccer task requires a large suite of different motions, including straight walk, turns, kicks, and handling of obstacles (walls and other robots). The goal of this research is to implement an algorithm to evaluate the effects of different kick motions and to select an optimal strategy in a play situation. Additionally, I will look at learning algorithms that would enable the Sony AIBO robot to adapt its gait over time and optimize the velocity and stability of its motion on a given surface.",,Carnegie Mellon,Computer Science
2003,Real Time Global Illumination of Deformable Objects,"I present a data-driven method for the real time rendering of deformable objects that accounts for global illumination effects on diffuse surfaces in low frequency lighting conditions. The approach is viable in situations where the space of potential object deformations is known beforehand, so that model appearance information can be pre-computed for a representative sampling of deformation states of the model. Data reduction, primarily via principal component analysis is performed on the pre-computed appearance data to determine a low rank approximation to the complete appearance space. Runtime computation of appearance data for an arbitrary point in appearance space can then be performed in real time in software or on programmable graphics hardware by taking a linear combination of a small number of basis vectors spanning the reduced appearance space.",Alumni Award for Undergraduate Excellence,Carnegie Mellon,Computer Science
2003,Pervasive Computing: Supporting Group Activities,Collaborative services need to balance user privacy and system effectiveness. I will explore the tension between these conflicting goals in the context of a group scheduler. My system will allow useful information (mutually enjoyed activities) to be extracted while revealing little personal information (preferences for all activities and people). Furthermore I will investigate possible attacks allowing reconstruction of sensitive data as well as potential countermeasures.,,Carnegie Mellon,Computer Science
2003,Scaling Properties of the Internet Graph,"As the Internet grows in size, it becomes crucial to understand how the speeds of links in the network must improve in order to sustain the pressure of new end-nodes being added each day. Although the speeds of links in the core and at the edges roughly improve according to Moore's law, this improvement alone might not be enough. Indeed, the structure of the Internet graph and routing in the network might necessitate much faster improvements in the speeds of key links in the network. In this paper, using a combination of analysis and extensive simulations, we show that the worst congestion in the Internet in fact scales poorly with the network size ($n^{1+\Omega(1)}$, where $n$ is the number of nodes), when shortest path is used. We also show, somewhat surprisingly, that policy-based routing does not exacerbate the maximum congestion when compared to shortest-path routing. Our results show that it is crucial to identify ways to alleviate this congestion to avoid some links from being perpetually congested. To this end, we show that the congestion scaling properties of the Internet graph can be improved dramatically by introducing moderate amounts of redundancy in the graph in terms of the edges between pairs of nodes.",,Carnegie Mellon,Computer Science
2003,An Alignment Algorithm for Example-Based Machine Translation,"In this increasingly integral world, language barriers often prove to be a noteworthy hurdle in human interaction. For this purpose, Example-based Machine Translation (EBMT) has been used extensively in translating a human language to another, due to its efficiency and minimal need of linguistic knowledge. However, the accuracy of the translations by has been hampered by mediocre alignment algorithms that result in limited high-quality mappings of identified phrases. The goal of this research is to significantly improve the quality of phrasal alignments, while conserving the efficiency of EBMT, to ultimately enhance the accuracy of translation.",,Carnegie Mellon,Computer Science
2003,Logic Minimization Using SAT Checkers,"Post-Doc Advisor: Dr. Michael Theobald We investigate new techniques for solving the two-level logic minimization problem: Given a Boolean function, find the smallest OR-of-ANDs expression that represents it. This is a problem of both theoretical and practical interest. It arises in several fields of Computer Science, such as digital design, reliability analysis and automated reasoning. This project explores a new approach that involves the use of SAT checkers: programs that accept as input a Boolean formula f using only ANDs, ORs and NOTs, and determine whether the given formula f is satisfiable or not. To some, this approach might seem counter-intuitive; the SAT problem is NP-complete, and so it may appear inefficient to base our techniques on it. Thus, it is a surprise to learn that the SAT-based approach has shown considerable promise in our work. As a by-product, our research has yielded a number of ideas and observations that can be explored in other contexts, such as hardware verification, software verification, and QBF checking (satisfiability checking for a class of formulae that is 'harder' than what SAT checkers can handle).",Allen Newell Award for Excellence in Undergraduate Research,Carnegie Mellon,Computer Science
2003,The Honeywell Triplex Sensor Voter,"The Honeywell Sensor Voter Project involves formally verifying desired properties of the Triplex Sensor Voter, a real-time flight control device made by Honewell Inc. Honeywell Inc. was interested in verifying certain safety properties of this device. In order to do this, they had previously used random testing and simulation and other traditional verification mechanisms. Although these approaches provide examples of scenarios in which the system's safety properties hold, they do not guarantee that the safety properties hold in a sufficiently comprehensive set of scenarios. In our case study we explore the use of model checking, a formal verification mechanism, to provide this guarantee. Apart from providing such a reliable means of verification, the process of model checking also aided in refining and disambiguating the Triplex Sensor Voter's underlying algorithm. Our case study also demonstrates adapting model checking, a tool built for discrete systems, to verify properties for a real-time system. Although previous work has been done in this area, we are the first to verify these properties using Symbolic Model Verifier and only the model checking structures defined in ""Clark, Grumberg, Peled. Model Checking. MIT Press, Cambridge Massachusetts Â©2001.""",,Carnegie Mellon,Computer Science
2003,Generalized Matrix Computation on Graphics Hardware,"The increased versatility of graphics hardware has allowed for general computation to be mapped onto the GPU. The main considerations that must be addressed before widespread adoption of the GPU as another computation processor are the availability of high precision formats and APIs that abstract away the details of the implementation. Recent graphics hardware has support for floating point data throughout the pipeline, so we turn to the second concern of providing an interface to the hardware that hides the specifics of implementing the functionality on graphics hardware, but still maintains performance. We then use this interface to implement non-negative matrix factorization when used for performing feature extraction to demonstrate the strengths of the library when run on current graphics hardware.",,Carnegie Mellon,Computer Science
2003,Adaptive Cloth Simulation,"Digital characters in computer animation are often draped in sheets of polygonal cloth. My current research deals with cloth simulation, the physically based modeling of cloth for realistic animation. Cloth simulation has traditionally been simulated on a static mesh composed of a uniform grid of nodes. Since a coarse mesh cannot generally capture fine wrinkles, realistic cloth simulation uses a fine grid of nodes that is expensive to simulate. However, fine wrinkles are often localized to small portions of a mesh, suggesting an adaptive method to locally refine the simulation as needed. Recent work has produced adaptive methods for simple models of cloth and related simulations. I hope to extend this work for adaptively simulating more realistic models of cloth.",,Carnegie Mellon,Computer Science
2003,Detecting Cheaters in a Distributed Multiplayer Game Environment,"Cheating is currently a major problem in today's multiplayer games, and the most popular cheat used involves having the client software render information which is not in the current game view. In a first person perspective game, this type of cheating would allow a player to see their opponents through walls. Currently, many people are studying how this type of cheating can be detected and prevented in the context of client-server multiplayer games, but no one is studying how this type of cheating can be detected or prevented in a distributed context. We are studying cheating behaviors in the context of a distributed publish-subscribe system. In this system, players create publications which describe where they are and what their actions are. Players also create subscriptions which register their location-based interests for other players in their vicinity. The first type of cheating involves a player who publishes themselves at multiple locations in the game world (i.e. ""cloning""), and the second type of cheating involves a player who subscribes to more information than is currently in their field of view. We believe that both of these types of cheating can be detected by using a machine learning algorithm to classify patterns of behavior based on previous publication and subscription histories.",,Carnegie Mellon,Computer Science
2003,Understanding the Interplay between Usability and Individual Privacy Preferences,"Lack of trust in the infrastructure has been reported by many as a major impediment to the broader adoption of e-Commerce and other Internet applications and services â e.g. (Georgetown Internet Privacy Policy Survey 1999, Wang 2001, JRC 2001, Zobel and Sadeh, 2001). Many users perceive solutions deployed over the Internet to collect, analyze and disseminate information as potential threats to their privacy, sometimes for good reasons and sometimes simply out of a lack of understanding of what these solutions actually do with their information. As the information and communication infrastructure helps mediate an ever broader range of activities, corporations, lawmakers, regulation agencies and consumer advocacy groups have to grapple with a broad range of ill-understood issues that directly impact peopleâs privacy. The explosion in mobile phone ownership, the emergence of authentication services such as .NET Passport or the deployment of âcontext-awareâ functionality that makes it possible to track peopleâs location and other aspects of their daily activities further exacerbates this situation. While it can be argued that consumers may benefit from disclosing some information about themselves (e.g. through greater degrees of personalization or context awareness), they also run the risk of seeing their information misused. The scenarios range from being bombarded with annoying location-sensitive ads to downright scary situations where sensitive financial, health or other information could fall into the hands of unscrupulous third parties. Within this context, black-and-white policies that would strictly limit the disclosure and use of private information would fall short, as they would close the door on a number of potentially appealing scenarios. Similarly, too loose an approach would likely fail to provide consumers with sufficient trust in the Internet environment. It would seem therefore that, in many situations, it would be best to provide consumers with a framework within which they could decide for themselves what the best tradeoffs are, namely what information to disclose to whom and under which conditions. This view, which has been advocated by many, is in line with the approach taken by the World Wide Web Consortium (W3C) and its Platform for Privacy Preferences (P3P) project, which is quickly emerging as the industry standard for providing users with more control over the use of their personal information (W3C-P3P, 2001). Under P3P, web servers send machine-readable privacy proposals to the browsers of users who attempt to connect to their site. The browser then compares the proposal with the consumerâs own privacy preferences and determines whether or not to proceed or possibly prompt the user for a decision. While well-intentioned, this approach creates a tension between privacy and usability. To what extent can we expect users to specify their privacy preferences? Do users know their preferences ahead of time and do they actually understand the implications of different privacy options? If users are not willing to sit down for hours, specifying their preferences, what alternatives do we have to capture these preferences? Is it possible to identify key factors influencing user privacy preferences (e.g. nature of information being disclosed, nature of the site the information is disclosed to, whether or not the information is stored in an identifying manner, whether it is shared with third parties, prior interactions the user might have had with the site, nature of the interaction between the user and the site such as browsing versus shopping, etc.). By attempting to answer these and related questions in a scientific manner, we hope to help lawmakers, regulation bodies, businesses, consumer advocacy groups and other relevant actors to make betterinformed decisions. In the process, we also hope to inform key standardization efforts such as those conducted by the Word Wide Web Consortium under its ongoing P3P project. ",,Carnegie Mellon,Computer Science
2004,Simulation of a Nuclear Magnetic Resonance Quantum Computer,"Many believe that classical computation has limited computational power, unable to quickly solve several interesting families of problems. Quantum computation is an alternate avenue of attack, it relies on the power of quantum mechanics to superpose and entangle information in hopes of gaining a computational advantage. There are several possible physical realisations of quantum computers being studied by the community. One possible realisation is a quantum computer built on nuclear magnetic resonance (NMR) principles. This thesis is intended to implement a simulation of NMR computer. The final goal is to construct a simulation that will take in an algorithm, process it into NMR primitives which are passed to a NMR simulator. The NMR simulation outputs the result which then has to be processed in same formalism as the input algorithm. At the highest level, the fact that the computation is being performed on an NMR computer will hide and should appear a black box. This simulation provides two major purposes. The first purpose is that it will quickly allow researchers to test their ideas for NMR algorithms on a simulator before taking the time to run the algorithm on physical NMR hardware. The second purpose is to be an educational tool, to visualise algorithms and more intuitively understand quantum algorithms.",,Carnegie Mellon,Computer Science
2004,Single Point of Contact Manipulation of Unknown Objects,"Working in the presence of uncertainty is a fundamental problem in robotics. We are developing a method for performing single point of contact manipulation on initially unknown rigid bodies. Our approach adapts methods which have been used in the context of other robotics problems to this particular domain. We consider a robot that learns about the world through force feedback from its manipulator, fiducial tracking of the object's motion, and visual detection of the object's silhouette. Based on the information obtained from these sources we wish to generate a sequence of manipulation behaviors which minimizes the expected time required to move the object into a desired pose. We are developing behavioral primitives which are robust in the presence of uncertainty, methods for representing and reducing that uncertainty during manipulation, and planning software that reasons about the relative values of the available behaviors based on current knowledge. Our experiments are performed with a simulated world and robot.",,Carnegie Mellon,Computer Science
2004,A Power Saving Technique for TCP Connections over Wireless Links,"The IEEE 802.11 wireless LAN protocol prescribes the use of a power saving mode in which the network interface card of a mobile host is turned off periodically to save energy. In practice, the beacon period is typically set to 100ms. For TCP connections, this choice of beacon period can cause observed round trip times to be rounded up to the nearest multiple of 100ms. This rounding effect can hurt performance dramatically, for example inflating the amount of time it takes to transfer a web page from an HTTP server to the mobile host, Through simulations and real world data, we show that it is possible to select a beacon period per TCP connection such that at least as much energy is saved as with the 802.11 power saving mode, but at much greater performance.",,Carnegie Mellon,Computer Science
2004,On the Hardness of Uniform Random Generation,"The ability to randomly generate combinatorial objects is the fundamental technique underlying cryptography. The ability to randomly generate distributions of ""hard"" problems is useful for encryption and security. In addition, algorithms to randomly generate certain structures can teach us more about the structures themselves: some properties of regular graphs have been discovered by analyzing algorithms known to randomly generate them. Despite this, there has been little general research into random generation algorithms. For example, it is unknown whether there is an efficient algorithm that will uniformly generate an NP-complete language. My thesis aims to study the questions: In general, how computationally difficult is it to randomly generate a distribution? Can any NP-complete problems be efficiently generated? And, how does the computational intractibility of a particular problem affect the difficulty of randomly generating instances of that problem?",,Carnegie Mellon,Computer Science
2004,Synthetically Real Graphics,"Synthetic Reality is a project recently started by Professors Seth Goldstein and Todd Mowry with the goal of creating metamorphic robotic systems that simulate real moving objects. These systems consist of small, independent modules, or .catoms,. each containing a processor, memory, a capacitor (for power), programmable magnets (for motion), and wireless communication capability. The surface of each catom has a graphical display, onto which appropriate textures can be mapped to simulate the surface of the real object that is being modeled. I am working on the problem of mapping textures to the surfaces of these catoms so as to minimize distortion, both in the view-independent and view-dependent cases. Since the first physical prototypes of the catoms will be the size of ping-pong balls, shapes to be modeled will potentially lose much of their topographic information. As a result, texturing is very important in maintaining an approximation of the original appearance of the model.",,Carnegie Mellon,Computer Science
2004,Toward a Complete Face Recognition System,"A user should be able to sit in front of a computer equipped with a web camera, and the program would be able to capture the photo of the user and through various modules, process the photo and name the user. Currently there are existing separate modules that perform separate functions. For example, the face classification module uses SSD as feature. An optional component of the research is to investigate other features to be used for classification, in particular the use of wavelets. This might improve the accuracy of the system. The significance of this research would be to provide a basis for future development of many applications, such as surveillance, digital personal assistant, and camera-equipped cell-phones, which require individual face recognition.",,Carnegie Mellon,Computer Science
2004,Surface Capture of the Human Hand,"Capturing the details of a moving surface with a system of cameras is a complex problem which has been receiving a great deal of attention in recent literature. Current methods for surface capture of human actors (""skin capture"") do not yield results which are sufficiently realistic for use in the depiction of characters for computer animation. At the same time, current techniques for character animation lead to large amounts of production effort and cost to achieve realism when the deformations of skin must be rendered close-up, particularly in regions such as the hands or face where deformation is elaborate and subtle inconsistencies with reality are easy for the human eye to spot. A skin capture system with the ability to produce an animated 3D model whose deformations are indistinguishable from the original actor's skin would enable animators to produce characters with realistic skin using less time and effort. We propose a new system for surface capture of the human hand, a subject which has proven difficult to depict in a realistic manner for character animation in films and computer games. Recent surface capture systems attempt to build an arbitrary surface from a subject about which little or nothing is known, and have trouble when confronted with a hand in a pose which produces an elaborate shape with many occlusions. In contrast to these systems, our approach will use structural knowledge of the human hand to very quickly estimate the position and shape of the surface of the skin and rapidly converge to an accurate representation of the current state of the actor's hand. The system will allow a new and unknown actor to set up, calibrate for their hand, and begin capturing in the span of a few seconds, much more quickly than typical setup times of several minutes for traditional motion capture systems. Finally, the method used will be generalizable for use in capture of other articulated subjects whose structural knowledge is available.",,Carnegie Mellon,Computer Science
2004,Visual Routines for Spatial Cognition on a Mobile Robot,"In his studies on visual cognition, Ullman (1984) proposed that high-level vision makes use of reusable elemental operators which may be sequentially composed into a ""visual routine"" specialized for a particular visual task. Seeking to explore Ullman's ideas in the domain of robotics, we have created a framework for robot vision programming which uses reusable operators to parse the visual world. These visual operators act on and transform between two different representations of the world: iconic (pixel-based) and symbolic (shape-based). The framework provides a powerful set of primitives for the construction of visual behaviors, while also providing a GUI which allows for the run-time visualization of intermediate representations. The framework has been implemented in simulation and on a Sony AIBO robot, to perform tasks such as tic-tac-toe board parsing and character recognition. Current research is focused on using camera information to generate a persistent top-down world representation, allowing for more elaborate spatial behaviors.",,Carnegie Mellon,Computer Science
2004,A Fast Counting Data Compression Algorithm,"I explore a new type of data compression based on storing frequency counts. Traditional compression algorithms use either pattern building or statistic methods. Statistical methods have always proven to be most effective (although also the slowest) and much is know about their theoretical performance. Not a lot can be said about the theoretical performance of pattern building programs, although in practice, they work well, and are widely used. Data compression is used substantially in audio, video, and anywhere that datasets are large and/or bandwidth/media are small. Improvements in algorithms and theory dealing with compression help make the most of resources that are already in place. This project focuses on compressing data storing and using byte frequency counts. For free the frequency counts give you the ability to use statistical methods, because if you know the number of times each byte occurs in the file, and hence the probabilities of occurrence. However, you also know those probabilities at each stage in the compression by writing a byte, decrementing your counter and re-computing them. Basically, by providing more information than in a strait statistical method, greater compression can be achieved. A first order method proved to be about 10% more effective in practice than Huffman encoding, and higher orders only improve the effectiveness. Also, theoretically, it should out-perform both statistical and pattern matching algorithms in all cases. This method of compression had provided for some insight into how files are constructed statistically, and lead to some interesting file analysis/type guessing tools. Another interesting result is that this method reduces general file compression to compression of many simple collections of 256 numbers that obey many nice properties.",Allen Newell Award for Excellence in Undergraduate Research,Carnegie Mellon,Computer Science
2004,A Partially Automated Proof of the Cantor-Bernstein Theorem,"AProS is an automated proof search system created in the department of philosophy at Carnegie Mellon which finds natural deduction proofs for assertions in first order logic. A distinctive feature of AProS is that it produces natural proofs, similar to those a human might produce. I plan to extend AProS to use it to prove mathematical theorems from appropriate axioms, with the particular goal of proving the SchrÃ¶der-Bernstein theorem from the axioms of Zermelo-Fraenkel set theory. Previous work has extended AProS to allow for metamathematical proofs (specifically proofs of GÃ¶del's incompleteness theorems and related theorems), but this involved coding domain specific heuristics into the overall AProS architecture. I will implement appropriate general heuristics for handling axioms, definitions, and functions. I conjecture that when applied to the specific axioms and definitions of set theory, with appropriate lemmata, a proof of the SchrÃ¶der-Bernstein theorem can be found. In keeping with the overall goals of the AProS project, this extension should generate natural proofs.",,Carnegie Mellon,Computer Science
2004,Detecting Opponent Roles in a Robot Soccer Domain,"Robot soccer provides a good domain for role-based opponent modeling because roles are well-defined and integral to the play system of the opponent. Specifically, I will detect roles and role transitions in the adversary team. This information can be used in other formation detection domains. Role detection is achieved with a combination of hand-coded rules, hidden markov models, and decision trees.",,Carnegie Mellon,Computer Science
2004,Augmented Reality Human Machine Interface for a Teleoperated Nano-scale Interaction and Manipulation System,"Perhaps one of the most challenging yet popular areas of research today is that involving the study of very small objects. Due to their small size, handling and interacting with these objects is beyond the capabilities of human sensing and precision. To overcome the scaling barrier when manipulating objects at the nano scale, teleoperation providing direct control of a nanorobot with visual and haptic feedback seems to be a reasonable approach. Hence, this thesis is focused on developing a Human Machine Interface (HMI) for real-time imaging in the nano world during nano manipulation/interaction with an Atomic Force Microscope (AFM) probe. This tool would provide scientists with the opportunity to learn more about the complexities of the micro/nano world. In addition, since it would facilitate the manipulation of micro/nano scaled objects, it would have applications in many different fields, including biology, chemistry, and physics.",Alumni Award for Undergraduate Excellence,Carnegie Mellon,Computer Science
2004,"FrontDesk: An Enterprise Class Web-based Software System for Programming Assignment Submission, Feedback Dissemination, and Grading Automation","The process of receiving a student submission and coordinating a course-wide assessment of user submissions is not a difficult conceptual problem. Many ad hoc solutions are implemented in a variety of courses which attempt to implement the process through various, disparate techniques. However, the process becomes much more difficult to manage when the goals of providing a common interface, rich user feedback, multiple submissions methods, section and submission group management, and distributed objective testing are considered worthwhile. In order to implement a system that meets these requirements, enterprise development practices and tools must be employed. FrontDesk is a Microsoft ASP.NET based Web application that attempts to improve communication and understanding between students and course staff through a rich submission and grading common environment. The focus of this paper is a more detailed, case-study style description of the problem domain and what we believe to be the best ways to solve each facet of the larger problem. In addition, we provide justification for each feature in FrontDesk and its benefit to both course staff and students. We will explore how Microsoft .NET distributed application technology applies to our specific area of development. Specifically, we will describe the enterprise architecture of the system and how certain attributes of the problem domain influence specific design decisions. Lastly, we will explore how the final solution solves the basic requirements of the robust submission and grading environment discussed in the beginning of the paper through analysis and user feedback.",,Carnegie Mellon,Computer Science
2004,Machine Learning Classification of fMRI Data in Semantic and Syntactic Tasks,"The advent of functional Magnetic Resonance Imaging has been an incredible boon for cognitive scientists in the last decade. By applying the power of computational processing to the tremendous quantity of data generated by fMRI studies, brain activity can be analyzed at a finer granularity than previously possible. My current research involves using the naive Bayes classifier to classify fMRI data between semantic and syntactic tasks. The goal of this research is to highlight the critical differences between the activity patterns seen when classifying isolated words as opposed to simple sentences. Through the analysis of classifier performance, I hope to lend new insight to the current understanding of linguistic comprehension. At the same time, methodology for such analysis must be constantly refined to eliminate confounding influences and lend credence to the results of classification.",,Carnegie Mellon,Computer Science
2004,Data Classification and Relaxing Storage Requirements,There have traditionally been certain guarentees assumed about data stored on disk: that it will remain unchanged and accessible. My work is exploring applications which can continue to behave correctly even when the disk does not meet these conditions. The application's data is classified based on the guarentees that the application requires and I will show an automated method for evaluating applications following changes to the disk's guarentees about its data.,,Carnegie Mellon,Computer Science
2004,Quantum Information and Game Theory,The main aim is to identify areas in game theory where concepts from quantum information may be useful and vice versa. I have attempted to understand the nature of correlations that exist in quantum entanglement and how they may be used as a resource in games.,,Carnegie Mellon,Computer Science
2004,Comparison-based Filesystem Verification (The NFS Tee),"This work explores a particular approach to testing a developmental filesystem -- comparing its behavior with that of a reference implementation in a live environment, that is, one consisting of real users, precious data and non-synthetic workloads. We believe such a configuration has the potential to offer unique insight into the system's behavior and consequently aid the development effort. The bulk of the project's work will lie in the construction and subsequent experimental usage of a tool, dubbed the NFS Tee, which is hoped to become a key debugging component in the Parallel Data Lab's Self-* Storage project.",,Carnegie Mellon,Computer Science
2004,Visual Validation of SSL Certificates in the Mozilla Browser using Hash Images,"Many internet transactions nowadays require some form of authentication from the server for security purposes. Most browsers are presented with a certificate coming from the other end of the connection, which is then validated against root certificates installed in the browser, thus establishing the server identity in a secure connection. However, an adversary can install his own root certificate in the browser and fool the client into thinking that he is connected to the correct server. Unless the client checks the certificate public key or fingerprint, he would never know if he is connected to a malicious server. These alphanumeric strings are hard to read and verify against, so most people do not take extra precautions to check. My research centers on building the visual hash output (image) into the Mozilla open source browser. Using a hash algorithm, a unique image is generated using the fingerprint of the certificate. Images are easily recognizable and the user can identify the unique image normally seen during a secure AND accurate connection. By making a visual comparison, the origins of the root certificate is known.",,Carnegie Mellon,Computer Science
2004,From Typed Assembly Language to Proof Carrying Code,"Typed Assembly Language (TAL) and Proof Carrying Code (PCC) are two assembly languages with additional markup that can certify that the binary code is safe to run. Although their goals are the same, they are not similar in their approaches. Furthermore the approaches taken by compilers to generate these binaries are very different, and hence compilers tend to target only one of these binaries, and are difficult to retarget. The goal of the research is to specify the translation from TAL to PCC such that any safe program in TAL will be a safe program in PCC with equivalent behavior. An additional goal is to implement an x86 version of the translator.",,Carnegie Mellon,Computer Science
2004,Behavior Programming Language and Automated Code Generation for Agent Behavior Control,"Behavior-based agents are becoming increasingly used across a variety of platforms. The common approach to building such agents involves implementing the behavior synchronization and management algorithms directly in the agent.s programming environment. This process makes it hard, if not impossible, to share common components of a behavior architecture across different agent implementations. This lack of reuse also makes it cumbersome to experiment with different behavior architectures as it forces users to manipulate native code directly, e.g. C++ or Java. The goal of the project is to provide a high-level behavior-centric programming language and an automated code generation system which together overcome these issues and facilitate the process of implementing and experimenting with different behavior architectures. The language is specifically designed to allow clear and precise descriptions of a behavior hierarchy, and can be automatically translated by our generator into C++ code. Once compiled, this C++ code yields an executable that directs the execution of behaviors in the agent.s sense-plan-act cycle. We have tested this process with different platforms, including both software and robot agents, with various behavior architectures. We experienced the advantages of defining an agent by directly reasoning at the behavior architecture level followed by the automatic native code generation.",,Carnegie Mellon,Computer Science
2004,TaskPort: A Task Management Interface in an Intelligent Cognitive Assistant System,"As technology evolves, people are pressed to keep track of a growing number of everyday tasks. Today's businessmen, managers and professors are spending more and more time on routine tasks such as answering emails, scheduling meeting, allocating resources and updating websites. RADAR (Reflective Agent with Distributed Adaptive Reasoning) is a software-based cognitive personal assistant in development by Carnegie Mellon University. RADAR can both save time for its user and improve quality of decisions by automatically managing the user's tasks and adapting to the user's preferences. The task manager is an essential component of the RADAR system that manages high-level tasks and coordinate communications among other intelligent assistants in a personal space. The task manager interface is a dashboard that allows users to browse, create, and modify tasks. It also obtains status from other task assistants and monitors feasibility of tasks. By exploring various techniques used in the task manager interface, we will better understand the elements that provide an edge in efficiency and productivity for the user. We will also discuss the possibility of extending the task manager interface in a ubiquitous environment.",,Carnegie Mellon,Computer Science
2004,Extended Filesystem Functionality for Self-Managing Storage System s,"Current large scale storage systems are very expensive because of the human element. The current industry estimates are that for every $1 spent on hardware, $6-$10 are spent paying people to install, manage, and maintain the hardware. In an attempt to lower the human cost, the Parallel Data lab is taking a step back and designing a new storage system from scratch that tries to be self administering. My part in this system is to determine if an existing filesystem called S4 is capable of being modified to support the ambitious requirements of the rest of the system. My primary goal is to modify S4 to export new functionality such as the ability to enumerate its files as well as allow for copy on write clones over ranges of files.",,Carnegie Mellon,Computer Science
2005,Type-Safety of Standard ML,"While a lot of research has gone into the study of type-safety, it has been done with simple contrived languages. There has never been a rigorous formal proof of this important property for any real language. Some prior work has gone into providing a definition of Standard ML, but it is known to contain bugs and was done entirely by hand. This could be greatly improved upon by a machine checkable proof of type-safety for Standard ML. Twelf is a formal logic tool designed for this purpose. The Harper-Stone definition of Standard ML has been formalized in Twelf and a proof of type-safety will be given.",,Carnegie Mellon,Computer Science
2005,Perceived Gender Similarities and Differences in Confidence Levels Among Computer Science Undergraduates,"This study is part of an ongoing line of investigation into the culture of computing amongst Computer Science undergraduates at Carnegie Mellon University. This study, focusing specifically on confidence, is looking especially at gender similarities (which have often been ignored in favor of examining gender differences), in an attempt to dispel some common stereotypes that surround women and men in computing fields. A set of interview transcripts has been studied and found that men and women share many factors relating to confidence levels. Our findings, so far, support the recent findings of Blum and Frieze, who found many similarities in how undergraduate men and women relate to computer science here at Carnegie Mellon University. Based on insight gained from our investigation of student interviews, suggestions are made to enhance the learning experience for undergraduate women in the computer science major. Ultimately we hope our suggestions may benefit all students.",,Carnegie Mellon,Computer Science
2005,Improving the Filtering Quality of Selective Dissemination of Information by Observing User Task Behavior,"A selective dissemination of information (SDI) service alerts users to latest documents in their field of interest. SDI helps users cope better with streams of incoming massive amounts of documents by filtering out uninteresting documents. Existing SDI filtering mechanisms typically use user feedback based on a binary 'interesting/not interesting' decision per document. However, in some situations the user will use information from the document in a structured way to complete a task, for example to fill out a form. In this work we describe a method of using user task behavior to improve the quality of SDI filtering. Our method creates, via machine learning, information extraction models of the information that the user is interested in. We then use these models on incoming documents to extract parts of the document that are likely to be of interest to the user. The results of the extraction are added as features into the filtering algorithm. We describe some experimental results of this method that demonstrate improved filtering performance of the SDI system (9%). We then describe the conditions where this method may be applied.",,Carnegie Mellon,Computer Science
2005,Using Objects of Measurement to Detect Spreadsheet Errors,"There are many common errors in spreadsheets that traditional spreadsheet systems do not help users find. This paper presents a statically-typed spreadsheet language that adds additional information about the objects that spreadsheet values represent. By annotating values with both units and labels, users denote both the system of measurement in which the values are expressed as well as the properties of the objects to which the values refer. This information is used during computation to detect some invalid computations and allow users to identify properties of resulting values.",,Carnegie Mellon,Computer Science
2005,Dynamic Texturing of Botanical Environments,"Data-driven animation techniques enable real-time synthesis of physically based motions without the cost of run-time simulation. The overhead costs of precomputing motion and lighting for a variety of objects, behaviors, and environmental conditions are large, but tolerable given the performance benefits at run-time. Since a primary goal of physically based modeling is to synthesize real world environments, a natural extension of precomputing and synthesizing complex behaviors for simulated objects is to place the objects in scenes representative of their natural environments. However, the cost of assembling and precomputing physical states for an environment scales as the environment grows in size and complexity, and the reuseability of the data diminishes. We provide techniques for synthesizing expansive, interactive environments that maintain the low run-time costs of data-driven animation without incurring the overhead costs of building and simulating large scenes. We utilize a small library of data-driven irises and an extension of Wang Tiles to generate a large field of irises that can be animated in real-time under varying wind and lighting conditions.",,Carnegie Mellon,Computer Science
2005,Perception of Affordances by a Mobile Robot,"In this project, we present a novel approach to robot mobile programming using the idea of affordances. Coined by the psychologist J. J. Gibson, affordances are descriptions of relationships between the robot, the object, and the environment they share that determine sets of possible actions that can be performed within that environment. In this framework, which is implemented using the Tekkotsu framework on the Sony AIBO dog, objects such as balls and lines are represented as primitives that can be seen, touched and manipulated with. The manipulation of the object then becomes a straightforward application of affordances. For example, a ball affords different pushing schemes based on its size: a large ball affords pushing using the dog.s chest, while a small ball affords pushing using the dog.s paw. Internally, each affordance is implemented using the following components: a finite state machine, an achievability code, and a return code. An achievability code informs the programmer how realizable an affordance.s action is prior to the programmer executing the action, while the return code informs the programmer how well the affordance perfomed its action. A set of affordances is generated by an affordance recognizer at each time step. The programmer only has to select which affordance to execute, alleviating him of the low-level details such as coordinate systems and mechanical limits associated with the robot.",,Carnegie Mellon,Computer Science
2005,Real-Time Texture-Space Radiosity,"This research presents a new texture-space algorithm for computing real-time radiosity in static scenes containing both dynamic, high-frequency local lights and low-frequency environment lighting. This method only requires a texture map parameterization of the target model, which already exists for most models for the purposes of tangent space bump mapping and other techniques. The algorithm presented generates a solution to the scene's diffuse global illumination by iterative solution of the Neumann form of the light transport equation, which enables us to dynamically simulate photorealistic illumination effects such as interreflection and occlusion. The scene radiance and corresponding transfer coefficients are efficiently encoded in texture space, which allows us to leverage graphics hardware to perform rapid Monte-Carlo sampling of the hemisphere at each texel, yielding an approximate solution to the transfer problem. A key focus of this research is on the design and implementation of efficient sampling techniques to speed convergence at the low sampling and iteration rates required for real-time applications. The algorithm currently makes use of low-discrepancy sampling techniques and spatio-temporal filtering to achieve efficient iterative transfer while maintaining a low level of noise. Real-time performance on modern graphics cards is demonstrated, and an analysis of the convergence of the solution under different sampling and filtering techniques is also provided.",Allen Newell Award for Excellence in Undergraduate Research,Carnegie Mellon,Computer Science
2005,Non-holonomic Trajectory Planning for High Speed Navigation,"In this thesis a new approach to high-speed navigation is presented. The limitations of traditional path tracking techniques have negative implications for the high-speed traversal of rugged terrain. Through the investigation of trajectory generation that is reactive and stable, the requirements for path tracking and obstacle avoidance at high speed can be determined. A reactive swerving approach, as implemented, allows autonomous vehicles to achieve higher speeds on rougher terrain. An analysis of the implementation of such an approach demonstrates the importance of dynamics and terrain-vehicle interactions at high speed. This approach to high speed navigation offers the possibility of broadening the application and use of autonomous vehicles in the world.",,Carnegie Mellon,Computer Science
2005,Classification of Examples by Multiple Agents with Private Features,"We consider the problem of classification where relevant features are distributed among a set of agents and cannot be centralized, for example due to privacy restrictions. Accurate prediction of the output class is difficult for an isolated single agent because the target concept may involve features to which the agent does not have access. To increase prediction accuracy, a learning algorithm is required in which agents collaborate to classify new examples, while preserving the privacy of their local features. We formalize this problem as the distributed classification task. We introduce a novel distributed decision-tree inspired algorithm for such tasks named DDT. One of the key ideas in DDT is that agents can communicate the information gain of a private feature without revealing the semantics of the feature or its actual value. We present empirical results in a calendar management domain where software assistant agents classify new meetings as .likely to be difficult to schedule. using private features such as each attendee.s willingness to attend the meeting. We show empirically that our approach outperforms a single agent learner and performs as good as a centralized learner with hypothetical access to all the features.",,Carnegie Mellon,Computer Science
2005,New Algorithmic Techniques for Generalized N-Body Problems,"This research examines existing methods and contributes new techniques for solving generalized N-body problems. These problems are characterized by functions of pairs or n-tuples of points in a metric space, and include fundamental problems in computational physics (electromagnetic and cosmological simulations), computational statistics (nonparametric density estimation and n-point correlations), computational geometry (all-nearest-neighbors problem and radial range-searching), database systems, computer graphics, and computer vision. In solving this important class of problems, I hope to provide a unified approach providing performance scalability with respect to the number and the dimensionality of data points and adaptability to arbitrary data distribution. First, I propose a new fast kernel density estimation algorithm combining two successful approaches in reducing the computational cost involved in nonparametric density estimation: a fully-recursive approach utilizing adaptive hierarchical data structures in computational geometry (dual-tree recursion) and an analytical approach in approximation theory (fast multipole methods). The technique developed here is general enough to be applied to other problems in which fast evaluations of ""pair-wise"" kernel functions are required. Secondly, our experiments have shown that tree-based N-body algorithms are highly adaptive to any data distribution and offer superior performance over those using other structures such as simple grids. I propose new tree data structures that will speed up the ""branch-and-bound"" searching involved in tree-based algorithms. In demonstrating the effectiveness of the newly developed techniques, I will provide experimental results against current state-of-the-art algorithms.",,Carnegie Mellon,Computer Science
2005,A Survey of Mechanisms for Language Extensibility,"We consider the problem of designing a language to allow incorporation of new features into itself from user source code, including combination with other languages. With this goal, we offer several major features that would contribute to such a language, and survey some works addressing these features. We conclude by suggesting aspects of the system that may require further research.",,Carnegie Mellon,Computer Science
2005,Reliable Rock Detection and Classification for Autonomous Science,"Current planetary rovers rely heavily on explicit instructions given by operators on Earth, wasting a great deal of bandwidth. An onboard science autonomy system is necessary to give rovers an understanding of science data, so that they may make informed, autonomous decisions, allocate bandwidth intelligently, and respond to new discoveries. A reliable rock detector is the first step in understanding local geology and creating such a system. The detection of objects in natural scenes is an open problem that has been addressed by many computer vision and machine learning researchers. It involves a difficult combination of image processing, the use of subtle visual cues, and domain knowledge about the world. A three stage hybrid architecture is presented that handles data from multiple sources and combines computer vision and machine learning techniques to achieve segmentation, detection, and classification of rocks. Results using ground-truthed data from the Atacama Desert in Chile suggest a successful system design.",Alumni Award for Undergraduate Excellence,Carnegie Mellon,Computer Science
2005,Automatic Digicromatography: Colorizing the Images of the Russian Empire,"A hundred years ago, before color photography existed, a Russian scholar Prokudin-Gorskii attempted to create color photographs by taking three black and white exposures of the same scene through three filters: blue, green and red, to be displayed using a special triple lens projector. Today, a little less than two thousand negatives survive and are available in digital format on the Library of Congress website. A color image can be reconstructed from a three-channel negative by manual alignment and careful color adjustment, termed digicromatography. However, the manual process is too time-consuming to be used on the entire collection, so an automated approach is needed. In this work we have investigated several ways of automating digichromatography. After the initial automatic color alignment using the Gaussian pyramid and affine warping, there are three types of image artifacts that must be fixed: 1) local level artifacts such as extreme red/green/blue spots from dirt particles on the negatives; 2) mid-level artifacts such as gradual region color change; 3) global artifacts such as unnatural or bleak color in the whole picture. We have applied different image processing techniques for each of these types of artifacts. Our completely automatic approach demonstrates promising results on many of the images.",,Carnegie Mellon,Computer Science
2005,Authentication and Access Control in Multi-agent Systems,"In a multi-agent system that is dedicated to personal task management, information flows within the system is usually sensitive and should be accessible by only a limited set of people. Some unique properties of the system raise different engineering challenges for the design and implementation of security and access control mechanism: Interpretations of information on different agents have different level of granularity and it can flow through nodes belong to different entities. We propose two design principles for determining when and how access control is enforced: (1) perform access control as early as possible (2) by using all information the system can obtained at that time. In this research, we focus on the RADAR Project as an example of such multi-agent systems. RADAR (Reflective Agents with Distributed Adaptive Reasoning) is a software-based cognitive personal assistant that helps people manage their routine tasks such as answering emails, scheduling meetings, and updating websites. In order to complete the tasks, agents in RADAR communicate with each other to obtain task-related information. In this paper, we describe how we apply the proposed design principles by implementing two levels of information access control policies in RADAR. The policies are configurable and can be applied efficiently in any multi-agent systems.",,Carnegie Mellon,Computer Science
2005,Towards a General Knowledge Representation Language,"In the history of AI, there have been many different knowledge representation languages for different purposes: production systems for fine-grained control, logics for propositions, frames for categories, programming languages for abstract procedures, Bayes nets for probabilistic reasoning, and others. Each language is good at representing certain types of knowledge (e.g. uncertainty, groups, patterns), but bad at representing many others. A complete AI agent, however, must be able to represent all major types of knowledge. This thesis has begun to address this problem of designing a general knowledge representation language. The first part of this thesis organized the research program. We first identify the major types of knowledge. We then present insights on knowledge representation from many disciplines (cognitive psychology, linguistics, common sense reasoning, etc.). Many such ideas clash with the current paradigm: human knowledge is context specific, three-year olds can reason in second-order logic, the Tower of Hanoi is an atypical problem, and first-order logic is a small subset of English. We present benchmark problems for measuring the capabilities of proposed languages. Finally, we identify the strengths and weaknesses of the major existing knowledge representation languages. For the second part of this thesis, we developed a new knowledge representation language (or, more aptly, cognitive system) that aims to integrate as many types of knowledge as possible. The system draws ideas from many existing languages. It aims for the declarative expressiveness of logics, the conceptual clarity of frames, the fine-grain control of production systems, and the abstract procedural organization of programming languages. We present an implementation of the system, analyze its capabilities, and demonstrate it on a set of knowledge representation problems.",,Carnegie Mellon,Computer Science
2005,An Efficient Implementation of the AKS Polynomial-Time Primality Testing Algorithm,"In their ""PRIMES is in P"" paper, Agarwal, Kayal, and Saxena (2002) provided a deterministic, polynomial-time algorithm for testing a number for primality. My work focuses on determining the real-world performance of the AKS algorithm. Implementation issues are discussed in detail, and empirical results presented. The performance of AKS is compared to other primality testing algorithms such as sieving, trial divisions, and Rabin-Miller.",,Carnegie Mellon,Computer Science
2005,Visualizing Robot Behavior with Self-Generated Storyboards,"Achieving intelligent behavior on a mobile robot requires a mix of sensory processing, navigation, object manipulation, and human-robot interaction. Behavioral routines are often programmed as a collection of finite state machines, with states corresponding to actions, and state transitions triggered by asynchronous sensory events or timer expirations. Debugging these complex real-time behaviors or making them intelligible to a human observer can be challenging. Development of GUI tools can help programmers simplify this process. In this project, we develop a graphical ""storyboard"" representation for visualizing a robot's behavior over time. Such a representation has uses beyond debugging. It can be used to make a robot's behavior comprehensible to its users. And it can provide a visual record of the robot's ""performance"" on a task, suitable for publication or display. The tool allows user to manipulate state machine diagram and automatically generate a storyboard view, based on layout of the state machine, from robot's execution traces. The tool is implemented in Java on a PC, and communicates with a Sony AIBO robot dog running the Tekkotsu application development framework developed at Carnegie Mellon.",,Carnegie Mellon,Computer Science
2005,Obstacle Map Construction from Aerial Information for Unmanned Ground Vehicle Navigation,"Unmanned Vehicle navigation has received heavy attention from the robotics community. The ground view of terrain, however, is insufficient for many unmanned ground vehicle purposes. A vehicle navigating at high speeds in off-road environments may be unable to react to negative obstacles such as large holes and cliffs. The PerceptOR project attempts to complement the sensing capabilities of an unmanned ground vehicle with terrain elevation data gathered from an unmanned helicopter. This information can often be misleading. For example, the aerial vehicle will perceive tree canopies as solid, impenetrable obstacles, leading the ground vehicle to entirely avoid areas which it may have been able to navigate with relative ease. A body of water, on the other hand, will be falsely identified as ideal terrain for vehicle navigation. My research explores techniques to significantly improve automated obstacle detection by utilizing additional sensor information such as color and signal reflectance. I exhibit methods to represent this sensor data in such a way that a neural network can successfully train on a small set of labeled data in order to classify a much larger map. Additionally, I show how these algorithms can be customized for the intended vehicle's capabilities in order to create more accurate obstacle maps that can be then used for path planning.",,Carnegie Mellon,Computer Science
2005,BeatLib: A general-purpose beat detection library,"For most modern music, the concept of the beat or rhythm is important to both the understanding and description of the music. For automated music interpretation and analysis, an algorithm for determining the beat of a song is necessary. The ""beat"" or ""tempo"" corresponds to a periodic framework around which elements of the music are arranged. It is a perceptual construct recognizable to most people regardless of musical training. From an analytical standpoint, beat can be broken down into two values: the period (length of time from beat to beat) and the phase (location of first beat relative to start time of the music). Several algorithms have been designed to find the beat of music, but they tend to approach the problem from different and incompatible angles. A standardized interface library would allow the best aspects of multiple algorithms to be combined and leveraged against beat detection problems. BeatLib is a toolkit of interfaces and components allowing for detectors to be constructed in an interchangeable fashion. It serves as a common-ground platform for comparison testing, and the modular design allows for new systems to be built from previously-constructed detectors. The system uses messages that are passed along strongly-typed connections between data sources and data sinks. Using the BeatLib framework, we implement Scheirer's and Goto's algorithms for beat detection and compare the two against beat information assigned by a human listener.",,Carnegie Mellon,Computer Science
2006,Blocks World Vision for the AIBO Robot,"Robust visual object recognition is still a largely unsolved problem. On the AIBO robot, color segmentation is robust,so object recognition in the Tekkotsu visual framework is based on mono-chromatic objects whose appearance does not change with orientation. This research seeks to expand upon existing object recognition tools by developing ways to detect objects with more complex features, multiple colors, and appearances that change based on orientation. The research focuses on developing algorithms to identify brick-shaped objects in the image seen by the robot. One approach recognizes the edge features that are invariant in a brick object. Line extraction algorithms are developed and applied to the image, and possible bricks are then extracted from the lines in the image based on relative size and orientation of sets of lines. In a differing approach, knowledge of the colors of the faces of bricks is used to find adjacent blobs of color that could be brick faces. The corners of these blobs are extracted to find more precise information about the size and shape of the brick. These algorithms are tested on many different brick objects, and their applicability toward other regular shapes (cylinders and pyramids) is considered.",,Carnegie Mellon,Computer Science
2006,Haplotype Motif Partitioning for Association Studies,"Since the first full genome was sequenced in 1995, the amount of available genomic data has grown exponentially. Utilizing patterns of variation, called haplotypes, has allowed scientists to begin drawing correlations between an organism's genetic code and the characteristics that manifest themselves, i.e. hair color, height, tendency towards depression. While several methods for finding haplotypes have been explored, one model, the haplotype motif model, is especially promising. Motifs are intended to capture conserved variation while relaxing some of the constraints imposed by previous methods. The model is designed to test whether correlation information in haplotypes is lost by the more rigid models. Finding the minimum number of motifs is an APX hard problem. So, the focus of my research has been to find an approximation to the solution. One approach is to use an integer programming formulation of the problem. To test how well the approximation algorithm finds useful motifs, this paper looks at compression ability and performance in association testing. The results are compared to two haplotype block models.",,Carnegie Mellon,Computer Science
2006,How are Faces Special? Eccentricity Bias as a Feasible Computational Inferotemporal Cortex Organization,The investigation of face and object representation in the brain has recently been intensified as brain imaging technologies have opened up new possibilities. This work investigates the degree to which retinotopic and metabolic constraints influence representation of faces and objects in the brain. The metabolic constraints will be investigated by recourse to the efficient coding of faces and houses using ICA. The retinotopic constraints will be investigated by seeing how the eccentricity bias in retinotopic representations may segregate the neural representation of faces and houses both topogrpahically and in a way that is scale-invariant.,,Carnegie Mellon,Computer Science
2006,Interactive Ownership Type Inference,"Ownership types have the potential to strengthen encapsulation in object-oriented programming languages. However, annotating existing code can be tedious, and fully automated ownership type inference has not worked well enough, with the result that it is difficult to obtain the benefits of ownership types in practice. We address this problem with an interactive ownership type inference tool that obtains information from the programmer about intended encapsulation properties and use this to infer ownership types. This will allow developers to quickly annotate existing code, enabling them to obtain the benefits of ownership types.",,Carnegie Mellon,Computer Science
2006,Mixture Model for Approximate Inference in Bayesian Networks,"Bayes Net is a useful tool for reasoning under uncertainty. However, exact inference in Bayes Net is generally an NP-Hard problem. In this thesis, we propose an approximate inference algorithm which trains a hierarchical mixture model to answer marginal queries in a given Bayes Net. The mixture model can be thought of as a hierarchical clustering indexing structure which allows fast learning and inference. We use a large number of hidden classes for accuracy while taking advantage of the unlimited training data generated by the Bayes Net. This method can also be used to do inference in hybrid Bayes Nets which is a difficult problem in general.",,Carnegie Mellon,Computer Science
2006,Real-Time Visual Robot Detection and Modeling with Situational Awareness,"Visual object recognition and world state modeling is a challenging problem for a mobile robot. This problem is even more difficult on the AIBO robot platform, which must rely only on local sensor information retrieved from a single monocular camera with limited viewing angle. In the RoboCup domain, specifically the 4-legged league, teams of AIBO robots play soccer against one another. Our research focuses on the visual detection and motion modeling of teammate and opponent robots on the playing field in order to build a more accurate model of the game state. Modeling teammate and opponent robots allows for the creation and improvement of a wide range of game behaviors, which our research also focuses on. Some example behaviors include path planning and obstacle avoidance, passing between teammates, teammate coordination, improved defensive and offensive strategy and positioning, avoidance of penalties caused by pushing robots, and searching for and retrieving the ball.",,Carnegie Mellon,Computer Science
2006,Thumb Based Interaction Techniques for Input on a Steering Wheel,"As more systems that require user input are integrated into motor vehicles or carried in by the driver, it is necessary to investigate new interaciton techniques that are well suited for in-vehicle interaction. The systems today include navigation/information systems, MP3 players, satellite radios, and cellular phones, PDAs, and more will certainly be available in the future or carried into the vehicle by the user. Current interaction techniques in automobiles mostly use buttons, custom knobs or on-screen keyboards, which require the user's visual attention. A proposed alternative is voice recognition; but it has numerous drawbacks, including low accuracy in general, susceptibility to environmental noise, vulnerability to user differences, and a tendency to convey greater intelligence than warranted. In order for in-vehicle user input to be safe and feasible, it is necessary to investigate interaction techniques that allow the driver to focus on the road by minimizing required visual attention and movement away from the steering wheel. This research is investigating interaction techniques that rely on the use of the driver's thumb on a touch-sensitive portion of the steering wheel. Selection task and text entry tasks will be evaluated while stationary and during simulated driving. Selection task that will be tested include: clutching, relative displacement and dialing. Text entry task will be evaluated using gestural input techniques and also selection based soft-keyboards. The selection task and text entry task will be compared to current methods in terms of speed, accuracy and also impact on simulated driving performances.",,Carnegie Mellon,Computer Science
2006,Methods for Extracting Names from Websites Containing Lists of People,"For my thesis I have developed an automated system for extracting people's names from websites containing lists of people. The contents of these websites describe attributes common to the people listed. This public information has strategic value, such as demonstrating who tends to appear at similar events. Unlike traditional named entity recognition (NER) we are extracting names embedded in HTML without natural language context. We use a hidden markov model (HMM) to segment the document's HTML source in order to extract entire names. Engineering features for this classifier led us to several general types of features useful for segmenting text in structured documents. Rosters may order first and last names in many ways. A first/last classifier determines the ordering used by each document using dictionaries to provide partial knowledge of the distribution of names across token positions. The first/last classifier uses the two dimensional coordinates of text as it would appear when rendered by a browser in order to abstract away the HTML. The HMM segmenter was able to achieve 95% precision and 91% recall while the first/last classifier achieved 84% precision and 82% recall, on average in a corpus of 37 documents containing approximately 10,000 names.",,Carnegie Mellon,Computer Science
2006,Distributed Detection of New Virus Threats in Large Scale Networks,"The goal of this research was to explore the possibility of extending the ideas exposed by John von Neumann in the paper Probabilistic Logics and the Synthesis of Reliable Organisms from Unreliable Components to build a very large scale intrusion detector system able to detect new cyber-attacks with higher reliability than its components. Specifically, the objective of this project was to design an information processor made of many components networked in such a way that the probability of false positive and false negative is smaller than is the case of the components. The first phase of the work consisted of familiarization with the details of the paper. Von Neumann described a system of ""organs"" in a nervous system. These ""organs"" were unlike their analogues, the heart or lungs. Rather the term described a small unit or component. Each organ had some chance of misfiring, termed epsilon. As a result, von Neumann's paper discussed a method of merging data together from a variety of organs so as to reduce the effect of ""faulty"" data. In addition, his paper addressed the issue of different messages being detected by different sensors. Von Neumann provided a method for combining this information by using properties of large numbers to find the ""true state"" of the system. We are currently investigating whether von Neumann's nervous system structure can be applied to anti-virus detection. Unlike the nervous network established by von Neumann to transmit a single, binary single, the proposed virus detection network must make affordances for other critical pieces of data: multiple viruses/different signatures, time discrepancy, and virus spread. In our paper, we investigate possible solutions to these aspects of the application of von Neumann's work to that of a virus detection network. In particular, a virus detection system must make more contact with the world of anomaly (the main mechanism for that kind of detection), as realized by operating system calls. The main idea is to examine how detectors using such system calls can exchange information so that their aggregated probability of false positive and false negative is much smaller than their corresponding individual probabilities.",,Carnegie Mellon,Computer Science
2006,Linearity for Objects,"Linear type systems guarantee that no copies are made of certain program values. The Ego language is a foundational calculus which adds linearity to object oriented languages. Ego allows changes to be made to the interface of an object, such as the addition or removal of methods, as long as such an object is linear, i.e., there exists only one reference to it. However, this linearity constraint is often unwieldy and hard to program with. We extend Ego with a linguistic primitive for temporarily relaxing the linearity guarantee. Ego allows objects to be linear and enforces that only one reference exists such an object. We allow multiple references to linear objects in certain expressions by borrowing references to these objects. Borrowing annotates the type of the reference with a region, which is a unique token indicating where the reference was borrowed. We disallow references with types containing regions that are not currently borrowed. We use this to temporarily make multiple references to an object in a given expression but enforce that outside this expression only one reference exists.",,Carnegie Mellon,Computer Science
2006,Enhancing Motion Data with Head and Eye Motion,"Motion capture is a widely used method for synthesizing character animations exhibiting natural motion. However, in most cases head movement and facial expressions are not captured and this leads to zombie-likeness from the neck up for character animations generated using this method. I am developing a model that takes existing motion capture data and automatically generates natural head and eye movements using the character's posture and environmental interaction.",,Carnegie Mellon,Computer Science
2006,Managing and Monitoring Spectrum Usage in a Wireless Network,"Cheap and easy installation of the wireless devices enabled the large number of wireless network deployments in many places. Howeveer, since there are only three-non-interfering channels in 802.11, interference between wireless devices will be major problem in sustaining the high efficiency and stability of wireless network. To reduce the interference in network, it is crucial for each device to know other device's activity and configuration in network. For example, the administrator need to determine the interfering sets of the access points and activity level of each access point to adjust the configuration to reach the ideal environment. One of the suggested solutions to this problem is to monitor the wireless activity of the network by running configuration and monitoring software in the access points. Upon receiving the wireless activity in the database, the network administrator can analyze the wireless activity in the real time and configure the access points to increase the total efficiency of the network. The real time wireless activity monitoring is the crucial part to understand the problems in the wireless network environment.",,Carnegie Mellon,Computer Science
2006,"Dynamic Architecture Reconstruction with Java 2, Enterprise Edition","Successful software system development depends on making good architectural decisions early in the design process, which are difficult to change and most critical to get right. In this architecture-based development, one of the challenging problems is to ensure that the implementation conforms to the architecture. In order to determine or enforce consistency between the architecture and its implementation, this research describes a technique that examines and monitors a system's dynamic runtime behavior. From the observation, in principle, we can infer its dynamic architecture which involves changes in the overall system configuration or the progress of the computation. Furthermore, by introducing a technique that maps low-level system events to high-level architectural representation using Colored Petri Nets, we can have a more accurate image of what is going on in the real system. This research covers dynamic architecture reconstruction for J2EE(tm) platform based software systems to demonstrate the practical applicability of the technqiues.",,Carnegie Mellon,Computer Science
2006,Learning Others' Calendars,"This work develops a method for learning about the meetings in others calendars based on their responses during the process of meeting scheduling. We gradually build up a probability distribution of meeting types for each possible timeslot in another users calendar. As we gain more information about other users calendars through additional negotiations, for example over the course of a semester, this distribution converges on the other user's actual calendar. This information can be used to speed the meeting negotiation process or to help users schedule meetings to their own benefit. This work is designed to integrate into the CMRadar project.",,Carnegie Mellon,Computer Science
2006,Managing and Monitoring Spectrum Usage in a Wireless Network,"In a large wireless network deployment, there are many questions for which the answers are either very expensive or even impossible to find. For example: How does a network administrator determine if a drop in network efficiency of one access point is due to interference from nearby access points or increase in load?How does one access point A tell access point B that access point B is interfering with A's communication? How do I make the access points configure itself so I won't have to worry about these questions? Currently, to answer these questions, a network administrator must perform a very expensive site survey. While the site survey will inform the administrator about any network interference and help him or her configure the wireless network, it will not provide constant monitoring to inform him or her of a change in the wireless network environment.The Solution. Our solution to monitoring the spectrum usage of a wireless network and configuration of the access points is to put the monitoring software and configuration software on the access points themselves. This way, a network administrator could simply add new access points in any convenient area with a network connection and not worry about the new access points adversely affecting the existing deployment. The configuration software will retrieve the current picture of the wirelss network from the old access points and configure themselves accordingly. The monitoring software will build a local image of the wireless spectrum on each access point and share that image to the entire network to build a global tapestry of the wireless spectrum.",,Carnegie Mellon,Computer Science
2006,Autonomic Computing: Learning to Repair Systems Effectively,"The goal of this research is to integrate known supervised learning methods into an autonomic computer system. The current RAINBOW architecture models a system where an adaptation engine looks at the system state to assess problems and determines a proper course of action based on pre-programmed expertise. However, in circumstances where expert knowledge is not readily available for a system, learning proper actions without a priori knowledge is often more necessary in the real world. This research seeks to develop a learning engine to learn the proper adaptations to repair problems in systems. Specifically, through the use of reinforcement learning methods it is possible for a system to learn the proper actions by actually trying out actions and seeing how well they do. I will compare Q-learning, SARSA, and actor-critic learning on our RAINBOW simulated system. Each of these algorithms works differently on most data, so learning which is best. Since they approach reinforcement learning in a different way, comparing the success rate and speed of the algorithms on various scenarios will lead us to useful conclusions about their efficacy. My research aims to clarify the benefits and costs associated with these algorithms.",,Carnegie Mellon,Computer Science
2006,Adaptive LAN-to-Host Multicast: Optimizing End System Multicast via LAN Multicast Integration,"The Carnegie Mellon End System Multicast (ESM) group has developed and is improving upon a technology to stream video/audio data over the Internet using peer-to-peer technology. It has been proposed that current streaming protocols can be improved upon by allowing end hosts that tune into a given stream to forward that stream to other end hosts, enabling a bandwidth savings to the streaming source by distributing the cost of deployment among the end users. Many problems still remain in the contexts of robustness, resilience, and scalability. Currently, we encounter a problem when attempting to stream within a LAN using the ESM tree when we compare it to a widely used protocol, IP Multicast. In the former approach, multiple streams are forwarded within a LAN, namely one for each sender-recipient pair in ESM. By integrating IP multicast on the LAN, however, we can reduce the cost of an individual LAN by allowing for one stream to propagate throughout the entire LAN once a certain threshold of users is reached. The threshold at which to make this change, however, is unclear, and the improvements from this modified approach are yet to be measured. My research will focus on implementing this new approach and answering these questions.",Alumni Award for Undergraduate Excellence,Carnegie Mellon,Computer Science
2006,A Comparative Genomics Approach to Identifying the Plasticity Transcriptome,"The aim of this project is to use computational methods to analyze the set of genes expressed in response to neural activity. When a neuron receives a stimulus from seizure or learning and memory, the amount of certain proteins that the cell produces increases through a mechanism called gene expression. This change is governed by the binding of transcription factors to DNA at specific sequences near the gene. Using a probabilistic model and a comparison between human and mouse, we identified a set of genes with CREB, zif268, and AP-1 transcription factor binding sequences. The presence of those sequences are known in many cases increase gene expression and protein amount. This set genes provides information about the role of the transcription factors as well as a resource for biologists looking to build specific networks of protein activation. We found that the transcription factors CREB and zif268 are likely to bind near genes involved in regulatory networks and the change of neuron structure. These results are compared to studies of gene expression following seizure. This work is a crucial first step in using computational methods to study learning and memory at the level of the neuron.",,Carnegie Mellon,Computer Science
2006,Toward Efficient Proof Search for Linear Logic,"Linear logic is a refinement of a mathematical logic that disallows weakening and contraction. This enables linear logic to treat propositions as resources that are part of a state and are generated and consumed, thus allowing modeling of state. Linear logic is useful in any realm that deals with reasoning about state; this includes planning problems and verification of hardware, software, and security protocols. We demonstrate various methods for improving the speed of an automated deduction system for linear logic: filtering of sequents, exploitation of a recently developed notion of left or right propositional bias, and use of a weighting heuristic for fact selection. Each of these methods reduces the number of facts and iterations required by the prover to find a proof of a given theorem. Finally, we demonstrate that the resulting reduction in facts and iterations translates into a significant clock-time speedup, improving suitability for the various applications outlined above.",,Carnegie Mellon,Computer Science
2006,Event Representation in Knowledge Systems with Context Hierarchies,"Scone is a high-performance, open-source knowledge-base system intended for use as a component in many different software applications. The Scone architecture supports contexts, which allows efficient representation and reasoning about different states of the knowledge base. Events and actions are particular types of knowledge involving mulitple world-states. The thesis is intended to present a flexible and powerful event representation that may be used in natural language processing, plan reasoning and generation. The use of contexts in representing events will allow efficient reasoning, while the use of background knowledge will allow better verb understanding in natural language. The general event representation will have two associated contexts representing the before and after states of the world (or the preconditions and effects of the event). Events, like other entities in Scone, will be a part of a multiple inheritance network, allowing a specific type of event to be instances of many types. The virtual copy semantics of Scone, allows fast and efficient inheritance and reasoning of the before and after contexts. Compound events, or plans, involve a more complex interaction between the contexts of events. A sequential plan requires certain contexts to be merged, and certain roles to be unified. Again, Scone allows efficient ways of representing compound events. The thesis will explore the representation of plans and possible uses of it with different software applications.",,Carnegie Mellon,Computer Science
2006,The Impact of Abandonment in Multi-Class Priority Queues,"Scheduling in web applications has been successful and is an important technique used for improving the performance of web servers, routers, and other web applications. Many applications of different scheduling policies have been studied, such as time-sharing, and size-based policies, however most studies have assumed that users are infinitely patient. This is not true in practice, where congestion leads to user frustration, and eventual abandonment (such as hitting the reload button where opening a website). Abandonment may cause problems for many scheduling policies, because when a job abandons, any work performed on it so far is lost (wasted). If abandonment is controlled then less service will be wasted, and jobs that complete will have lower response times. Abandonment is typically dealt with using admission control, by allowing a maximum number of jobs in the system at a given time, and rejecting any jobs that arrive while the system is full. Even in simple queuing models analyzing the effects of abandonment is difficult under many scheduling policies. As a result we limit our focus to m-class priority queues and develop simple approximations that characterize the effects on performance of abandonment both with and without admission control. In addition to mathematical analysis, we are using simulation to validate the accuracy of these approximations.",,Carnegie Mellon,Computer Science
2006,Active Data Structures and Applications to Dynamic and Kinetic Algorithms,"Dynamic alogorithms are a class of algorithms proven particularly challenging to design, analyze and implement. These algorithms maintain the input and output relationship as the input undergoes changes. Unfortunately, most known techniques for designing and implementing such algorithms are very complex, as suggested by the minuscule number of successful implementations. Self-Adjusting Computation, a recent development that combines algorithmic and programming-languages techniques, has helped simplify the task of algorithms engineering for dynamic algorithms and create new possibilities in kinetic algorithms. To this end, I conjecture that problem in many domains can be solved more efficiently if the self-adjusting framework is supplemented with external data structures that perform orthogonally to the core system. As my thesis, I propose to investigate a class of data structures, akin to the retroactive data structures [Demaine '04], and incorporate it to the existing framework. This incorporation is expected to create new frontiers in efficient algorithm dynamization. Concrete examples include: dynamic/kinetic 3D convex hull and dynamic shortest-path.",Allen Newell Award for Excellence in Undergraduate Research,Carnegie Mellon,Computer Science
2006,Analyzing Mobile Sensor Placement for Distributed Object Tracking,"Mobile sensor networks have a variety of applications in security and surveillance. One of the most important considerations for such a mobile sensor network is the problem of how to distribute the sensors to maximize coverage of the space to be monitored and detection of targets. Here we compare several strategies for positioning of mobile sensors, including: stationary sensor placement, sensors that rotate in place, mobile sensors moving between predefined waypoints, and an active search. The active search is a strategy where the mobile sensors repeatedly move to the edge of the recently viewed space in order to view areas which have either not yet been observed, or which have not been observed in a certain amount of time. Data from previous target observations is used to learn a model of target motion, which can be used to predict target movements and areas that targets may be likely to appear. Tests are performed in simulation in order to evaluate the effectiveness of the different sensor positioning strategies and sensor models.",,Carnegie Mellon,Computer Science
2006,Covert Multi-Party Computation,"We introduce an extension of {\it covert two-party computation} (as introducted by von Ahn, Hopper, Langford in 2005), to multiple parties. {\it Covert computation} is a stronger notion of security than standard secure computation. Like standard secure multi-party computation, covert multi-party computation allow $n$ parties with secret $x_1$ through $x_n$ respectively, to compute a function $f(x_1,\dots,x_n)$ without leaking any additional information about their inputs. In addition, covert multi-party computation guarantees that even the existence of a computation is hidden from all protocol participants unless the value of the function mandates otehrwise. This allows the construction of protocols that return $f(x_1,\dots,x_n)$ only when it equals a certain value of interest (such as ``Yes, we want to collude with each other'') but for which {\em no party can determine whether anyone else even ran the protocol whenever $f(x_1,\dots,x_n)$ is not a value of interest.} Since existing techniques for secure function evaluation always reveal that both parties participate in the computation, covert computation requires the introduction of new techniques based on provably secure steganography. We introduce two plausible security definitions for covert multi-party computation and show that the first is too strong, but that its relaxation can be achieved by a general protocol.",,Carnegie Mellon,Computer Science
2007,Gaussian Naive Bayes Classifier with Smooth Basis Functions,"We explore another way to identify human mental states using the data from fMRI machine. An fMRI machine measures the changes in blood oxygenation level, which is also known as hemodynamic responses (HDR). Since a hemodynamic response corresponds to a blood flow, it is convincing that fMRI data should have smoothness property. In this thesis, we extend the traditional Gaussian Naive Bayes classifier by adding smooth basis functions into the calculation to capture the smoothness property of fMRI data. After some experiments, we do not see a statistically significant improvement by using basis functions. However, we investigate further into the questions why using basis functions did not help and in what cases where using basis functions could improve the classification.",,Carnegie Mellon,Computer Science
2007,Autonomous Discovery of Landmark Objects,"The goal of this project is to develop a method by which an autonomous mobile robot can discover and learn representations and locations for immobile, landmark objects within an unknown environment. These landmark objects are large, rigid objects within the environment which remain in relatively fixed locations and can later be used for localization or navigation. Examples of such landmark objects include furniture and large decorations such as paintings. This project attempts to bridge the gap between the problems of autonomous mapping and object recognition in mobile robots. Current mapping algorithms are able to robustly learn the physical structure of an environment and use it for localization, but are unable to learn about the higher level structure, such as which parts of the structure belong to individual objects. Additionally, existing object recognition methods work very well when given a good set of training images describing the objects to be recognized, but there are not many methods for easily obtaining such training sets. By having a mobile robot autonomously discover landmark objects we would be able to simultaneously learn about the higher level structure of the environment and obtain data which can be used by existing object recognition methods. This allows the robot to automatically learn about critical objects within the environment it has to work in, eliminating the requirement for any pre-programmed obj! ect recognition databases or environment maps.",Allen Newell Award for Excellence in Undergraduate Research,Carnegie Mellon,Computer Science
2007,Manipulation of Objects Using an AIBO,"The Sony AIBO robot is built like a dog, having four limbs, a head and a tail. Typically, all four limbs are used in locomotion, while the head is used to look at and grasp objects. The general approach to the AIBO's grasping an object requires positioning the object between the forelimbs, then grasping it with the head. However, present techniques involve specialized routines written for specific object shapes and domains (e.g., ball manipulation in RoboSoccer), where a fixed set of key-frames or fixed motion sequences are developed, which the AIBO performs without feedback, much like acting out a pre-defined script. This approach is difficult to generalize, and thus the AIBO is unable to adapt to new situations. This research focuses on general methods that will allow an AIBO to manipulate classes of objects, such as boxes, spheres, and cylinders. The manipulation techniques will be as general as possible, so as to allow the AIBO to act on a variety of objects. In particular, visual feedback will be used to guide the AIBO as it performs the desired action. This allows the AIBO to adapt to changing circumstances and subtle differences in the objects being manipulated.",,Carnegie Mellon,Computer Science
2007,Proof Triangles: Toward a Formal Theory of Mathematical Understanding,"Humans have developed great proficiency at creatively tackling mathematical problems. We seek to develop a computational model which encompasses many features of the human problem solving process. As a basic primitive for this model we have developed an object which we call a ""proof triangle"". A proof triangle for a theorem can be thought of as a triangle which has mathematical statements written on it. On its base the formal systems proof is written, and at the apex of the triangle are very short hints. The rest of the triangle is filled with more detailed hints which lead to a derivation of the formal proof. These objects encompass the knowledge required to guide an algorithm or human trying to solve the problem at hand, but not all information in a triangle may be required information. In addition to providing a formal definition of a proof triangle system, we also develop formal definitions for concepts such as analogies and the extent to which an algorithm understands a theorem T and a proof P of the theorem. Our definitions approach these ideas from a complexity theoretic standpoint. We believe such methods will be useful in studying the broader problem of ""understanding understanding.""",,Carnegie Mellon,Computer Science
2007,Extending Aura with an Augmented Reality Interface,"In a ubiquitous computing environment augmented reality would be an ideal choice for a display for the user. Unfortunately augmented reality is technically difficult and costly to implement even when the application is designed for its use from the ground up. However, many of the necessary devices for a low fidelity implementation of augmented reality are readily available in a ubiquitous computing environment. Our research focuses on using Aura, a ubiquitous computing framework, to marshal the available devices in the environment. These devices can then be connected within the framework to provide an augmented reality display to the user at the best fidelity possible, given the available resources in a user's environment.",,Carnegie Mellon,Computer Science
2007,Tracking for a Roboceptionist,"Currently, the roboceptionist's ability to decide whether or not to greet a nearby person is inadequate. Often the roboceptionist attempts to greet people who are not interested in talking back, but merely passing by. The roboceptionist may also attempt to greet people who are standing outside the nearby classroom-these people are not interested in responding either. While the roboceptionist is attempting to greet people who will not respond, other people who do want to talk to the roboceptionist may be waiting for a greeting. The problem is that the roboceptionist is spending too much attention on people who are not interested in interacting with the roboceptionist. Before the roboceptionist can reliably greet people, however, the roboceptionist needs to have a reliable way of tracking people. This project focuses on methods of person tracking using a laser range finder.",,Carnegie Mellon,Computer Science
2007,Robust Detection & Recovery from Service Disruptions in Distributed Systems,"Distributed systems are complex to design, build and debug. Components crash due to software bugs such as unchecked array bounds, logic errors, and unchecked return codes. Components hang due to deadlocks and resource leaks. Instead of relying on failure-proofing the services, we focus on detecting and restarting the failed components. With the concepts and benefits of restarting in mind, I will present a ""watchdog"" service for detecting and recovering from failed components in a distributed system. As a case study, I will describe the design and implementation of a watchdog service for a distributed storage system called Ursa Minor. Experiences and novel extensions will be highlighted.",Alumni Award for Undergraduate Excellence,Carnegie Mellon,Computer Science
2007,A Template-based Approach to Mobile Reminders,"Busy parents often do not write down short term reminders for one-time events, like returning videos or bringing snacks to a soccer game, because it takes too much time compared to relying on their own memory. These types of events are the most often forgotten because people accumulate so many of these small reminders. Parents want a way to quickly make reminders to perform some task at a particular time, location or while performing an activity, while minimizing the cost of entering them. Our research focuses on using cell phones as location- and activity-aware devices to collect information about what people are doing. Then we use machine learning techniques to predict and auto-complete reminders. Although the events and locations may differ on a family to family basis, within a family, members have evolved a consistent system for reminding each other. We collect data from each family and then train the model for each individually.",,Carnegie Mellon,Computer Science
2007,3Twelf: A Tool for Reasoning about Programs,"Computer programming involves a difficult degree of reasoning that requires students to rapidly acquire new ideas and skills quickly so that they may apply them to more advanced concepts. Allowing students to rapidly receive feedback about mistakes and misconceptions can aid in student learning. To this end, we are designing and developing a tool that will assist students and other users in learning to prove theorems about programming languages. This tool should be easy to use for novices and thus should be modeled on concepts and semantics that mirror the content of introductory programming language theory courses. This way, it can be used concurrently without any need for additional tutelage. We have modeled our prototype tool on the content of Carnegie-Mellon's Foundations of Programming Languages and hope that it can be used in other courses similar to this in nature.",,Carnegie Mellon,Computer Science
2007,Efficient Algorithms for Similarity-Enhanced Transfer in Peer-to-Peer Systems,"Our study focuses on the development of efficient algorithms for data delivery and file distribution within the context of Similarity-Enhanced Transfer (SET), an enhancement to peer-to-peer(P2P) file distribution systems in which peers that hold similar files that overlap with the target file being downloaded are made available as additional peer sources for these overlapping data segments. Current P2P systems do not take advantage of the possible benefits of enlarging the set of peer sources from which the original target file can be downloaded with additional peers that hold similar but not exactly equivalent copies of the target file. Augmenting the initial set of exact file sources with sources of similar files and taking advantage of exploitable similarity at the level of file chunks has the potential of speeding up file download times while improving the robustness and longevity of a given P2P file distribution process. Our work in particular contributes to the development of different file distribution algorithms than those currently employed by popular P2P file distribution systems. Current P2P file distribution systems such as BitTorrent employ game theoretic motivated algorithms and replication heuristics in the face of distributing files over the Internet to participating peers that are autonomous, self-serving and possibly dishonest. Our end goal is to instead develop file distribution algorithms for SET employed in more trustworthy peer-to-peer networks such as internal or private networks, storage area networks and networks of parallel storage devices where a degree of centralization, coordination and dependability is available. Our approach utilizes gossiping algorithms to broadcast a file being downloaded to all peer participants, with focus on optimal algorithms for the arbitrary multi-source broadcast and disjoint multi-source broadcast situations.",,Carnegie Mellon,Computer Science
2008,Online Fingerpointing: Just-in-Time Problem Diagnosis for Distributed Systems,"Distributed systems are growing both in size and complexity. In the event of a system failure, this makes it increasingly difficult for systems administrators to determine which component failed. Existing tools and algorithms have been designed to diagnose problems, but they rely on offline analysis. This work explores the possibility of online failure diagnosis that operates as the distributed system under observation is running. A framework for online fingerpointing is presented and evaluated.",,Carnegie Mellon,Computer Science
2008,Combining Wireless Network Emulation and Simulation,"When developing new wireless networking technology, researchers often use network simulators and testbeds to perform experiments that demonstrate the correctness of their wireless systems. These experimentation platforms are often inccaruate or unscalable. We present a general method of combining wireless network experimentation platforms in order to improve accuracy and scalability. This technique is verified using a proof-of-concept system that combines two existing network experimentation platforms.",,Carnegie Mellon,Computer Science
2008,An Authorization Logic with Explicit Time,"The problem of allowing access to data and resources without compromising their security is a fundamental one in today's increasingly networked society. We believe that proof-carrying authorization, with its basis in logic, can serve as a theoretically sound solution to the access control problem. But, for proof-carrying authorization to be viable, its underlying logic must be sufficiently expressive to support a wide array of natural access control policies, including single-use and time-dependent authorizations. In this project, we will design a linear authorization logic that permits reasoning about time-dependent authorizations. We will also formalize the logic's metatheory, including the admissibility of cut, in the Twelf logical framework and implement a proof checker for the logic. Finally, we hope to explore the non-interference properties of the logic and use them to construct simple policy analysis tools.",Allen Newell Award for Excellence in Undergraduate Research,Carnegie Mellon,Computer Science
2008,Visually Guided Manipulation Primitives for an Educational Robot,"Regis is a new prototype educational robot developed in the Tekkotsu lab at Carnegie Mellon. It consists of a 6 degree of freedom Lynx Motion arm which ends with a gripper and a 4 degree of freedom Lynx Motion arm that ends with a camera. The project aims to develop simple manipulation primitives like pushing and grasping for Regis with the aid of visual guidance, hence overcoming problems associated with open loop manipulations.",,Carnegie Mellon,Computer Science
2008,Are You Your Own Worst Enemy? Self-Interruption on the Computer,"Although much research has been conducted on the disruptive effects of external interruptions during task execution, few have studied why we interrupt ourselves - i.e. interruptions occurring without any active outside influence, or self-interruption. This work is aimed at understanding the causes and effects of self-interruption to inform the design of productivity enhancing end-user feedback. The causes and effects of self-interruption were assessed through a set of user studies and, based on the results, a feedback mechanism is proposed that should reduce the harmful effects of self-interruption.",,Carnegie Mellon,Computer Science
2008,Investigating the Use of Machine Learning in Go,"The game of Go is practically the Holy Grail for computer game playing due to its massive branching factor and difficult evaluation. The current state of the art computer program is only able to play at an amateur level. Additionally, the computer programs tend to have specific weaknesses which can be targeted by professional human players. For this project, we constrain the problem to 9x9 Go and investigate the use of Machine Learning methods to train an evaluation function. Both supervised learning and reinforced learning schemes are considered. The resultant evaluation function can be directly applied to the game through an Alpha-Beta search, but other methods of applying it, such as in a Monte Carlo Go implementation, are also investigated.",,Carnegie Mellon,Computer Science
2008,Multiple-Target Tracking Based on a Fully-General Data Association Model Using a Fourier-Domain Representation,"Multiple-target tracking (inferring the position of each target over time, where each target has a unique identity) based on measurements that provide only imprecise information about the position or identity of the target that generated the measurement is a difficult problem due to the inherent combinatorial complexity that arises from the ambiguity regarding the association of measurements to targets; tackling this complexity depends on the use of approximation algorithms. Although existing sampling-based methods allow highly general observation and data association models, in general there is no reason to believe the probability mass can reasonably be concentrated on a small number of samples, and consequently accuracy guarantees for such methods depend on maintaining a number of samples that grows exponentially with the number of targets. Recent work by Kondor et al. and Huang et al. demonstrated the applicability of group-theoretic methods, specifically band-limited Fourier-domain representation of distributions over groups of permutations, to multiple-target tracking problems, but their methods depended on a restricted data association model based on the concept of tracks. Unlike sampling-based methods, this Fourier-domain representation can represent diffuse distributions, which are believed to likely occur in these tracking problems. This work extends the approach of Huang et al. to support a more general data association model while also modeling imprecision and uncertainty in the position information from measurements.",,Carnegie Mellon,Computer Science
2008,A Hybrid Formulation of the Ordered Logical Framework,"The logical framework LF is a powerful tool for encoding and carrying out the metatheory of logics and programming languages in a mechanized way. However, current work on LF has yielded little support for the metatheory of certain kinds of logic that are useful for reasoning about state. One fruitful approach (for the case of linear logic) has been to use hybrid logic, inspired by Kripke modal logic and temporal logic, to give the metareasoning tool access to how the object language context is being manipulated. The goal of this thesis is to apply the same approach to ordered logic, an setting capable of expressing even more constraints.",,Carnegie Mellon,Computer Science
2008,Discovering Tractable Cellular Automata Questions,"Cellular automata(CA) are a powerful computational device. CA's are interesting in that computation on a CA is both local and massively parallel. Moreover, even for complex, Turing-complete CA's the rule set is small and easily implementable on hardware. Using prepositional logic with the ""steps to"" relation, we can ask rather complex questions about the CA state space, such as: ""Is this CA reversable?"" ""Does this CA have a three cycle?"" ""Is this CA surjective?"" We can represent the steps to and inequality relations as finite state machines over infinite words, and use simple algorithms to combine these FSMs into larger FSMs that represent general prepositional statements. However, combining these FSMs can cause them to grow exponentially (specifically, to negate a machine, we must first determinize it, which expands the machine by an exponential in the worst case.) The goal of this thesis is to catalog the prepositional statements about FSMs that are tractably answer!",,Carnegie Mellon,Computer Science
2008,Natural Language Processing with Knowledge,"This project examines the problem of extracting structured knowledge from unstructured free text. The extraction process is modeled after construction grammars, essentially providing a means of putting together form and meaning. The knowledge base is not simply treated as a destination, but also an important partner in the extraction process. In particular, the ideas are implemented as a module closely tied to the Scone Knowledge Base system. I demonstrate that with a reasonable knowledge base and general construction rules, one can easily extract structured knowledge. This project also explores partial matching, word sense disambiguation and generalization in the context of constructions.",,Carnegie Mellon,Computer Science
2008,Object Recognition Tools for Educational Robots,"SIFT (scale-invariant feature transform) features, developed by David G. Lowe, have been found to be robust to translations, rotations and scaling, and have become the solution of choice for many when dealing with problems of robotic vision and object recognition. The SIFT algorithm extracts significant features from an image, but additional software is needed to match these features against an image library in order to do object recognition. Further software is needed to construct and maintain this library. At present there is no open source tool to conveniently perform all these functions. This research aims to develop SIFT algorithms into tools that facilitate robotic object recognition for students in undergraduate robotic programming courses. This would involve allowing programmers to use and also to understand the basics of the algorithms behind the tool, and to make adjustments within the object recognition tool to accomplish their goals. As part of the research, the tool will also be evaluated within a real classroom setting when it is released for use in an undergraduate cognitive robotics course.",,Carnegie Mellon,Computer Science
2008,Characterizing YouTube Videos,"In this research project, we will attempt to make a case for end system multicasting (ESM) by analyzing traffic patterns for high-volume, high-bandwidth websites and services. ESM is a form of multicasting that only requires implementation by end users at the application layer, rather than at the routing layer. This allows for more widespread adoption than IP multicasting, while maintaining the majority of the benefits. ESM is currently being used for high quality video streaming, and so our analysis will focus on data behind popular video streaming websites, such as the fast-growing video site, YouTube. Our project will analyze and summarize traffic patterns behind both individual and collective user-generated video clips from YouTube. For my midterm report, I will describe the background behind the project, our program architecture behind data collection and post-processing, preliminary results demonstrating a case for ESM, and expected future results.",,Carnegie Mellon,Computer Science
2008,Wearable Context-Aware Food Recognition for Nutrition Monitoring,"DiaWear is a wearable food and activity recognition system to monitor and aid medical patients with respect to calorie intake and consumption. Such a system will be ideal for diabetes patients, as well as pancreatitis, and other patients with dietary restrictions. This system may also find use in fitness and weight management. Gathering data from sensors placed on different parts of the body using existing platforms such as the CMU e-Watch and the CMU ArmBand, and by using a PDA equipped with accelerometers, information regarding the activity and exercise of the user will be recorded and analysed via machine learning algorithms. A food recognition algorithm on the PDA will allow the system to take pictures and recognize certain learned foods on a plate and relay the corresponding calories. Knowledge of calories burnt via merged sensor data and knowledge of available calories via food recognition, will allow the system to monitor and assist the user regarding calorie intake a! nd management on a regular basis. Recorded data will also help medical professionals to provide more accurate forms of treatment and medications. As part of a first semester CS Senior Thesis project, the primary role of the student will be to develop an image recognition algorithm to detect food, and future work, if time permits, will comprise of integrating this algorithm into the above mentioned system of wearable sensors.",,Carnegie Mellon,Computer Science
2008,RAMS and BlackSheep: Inferring White-Box Application Behavior Using Black-Box Techniques,"We describe and evaluate a new technique for diagnosing performance problems in distributed systems in a scalable manner by exploiting and analyzing only local (i.e. intra-node) black-box system metrics, and inferring white-box application behavior. We study the novel method of correlating white-box application event logs with black-box system metrics to gain insight into the behavior of a distributed system, and validate our approach through experiments on the Hadoop open-source implementation of Google's Map/Reduce distributed programming model. We inject failures and real performance problems gathered from failure data recorded in Hadoop's bug database.",,Carnegie Mellon,Computer Science
2008,Assistive Technology for Learning to Write Braille,"If they are to play a meaningful role in modern society, people who are visually impaired need to obtain information in an effective and timely manner. Accessing information requires the ability to read and write fluently. The Braille language provides a mechanism for the visually impaired to be fully literate participants in modern-day society. However, learning to write Braille is a non-trivial process that often involves long hours of tedious work. Learning to write Braille is even more difficult for young blind children due to many factors such as the required physical and mental exertion, and the delayed feedback on what was written. To address these needs, the TechBridgeWorld program at Carnegie Mellon University (www.techbridgeworld.org) developed an Adaptive Braille writing Tutor (ABT) that uses audio feedback to provide guided practice for young children learning to write Braille. Through extensive interactions with the Al-Noor Institute, a school for blind children! In Qatar, we extend the capabilities and potential impact of this Braille tutor. This honors thesis in Computer Science enhances the ABT in three important dimensions: Relevance to the Arab world, methodology of software design, and motivational factor for the students. Our interactions with Al Noor revealed a need for expanding the vocabulary of the ABT to include Arabic Braille,and also a strong need for increasing the enthusiasm and learning-efficiency for blind children learning to write Braille. To address these needs we make three important enhancements to the ABT. First, we enable the tutor to provide guided practice for the Arabic alphabet characters in Braille and facilitate the interface between the Braille tutor and the screen reading software used at the Al Noor Institute. Next, we improve on the ad-hoc design of the ABT software components by combining research methodologies in Assistive Technology, Intelligent Tutoring Systems, and Artificial Intelligence to propose a principled redesign of the ABT software. Finally, we study the literature on Educational Game Design and create an educational ABT-computer game to increase the motivation of children learning to write Braille. The outcome of this project is an improved Adaptive Braille Writing tutor that enhances the state of art in educational technology for the visually impaired.",Alumni Award for Undergraduate Excellence,Carnegie Mellon,Computer Science
2008,Design-Code Verification: When Design Deviates from Code,"In the business of software today, developers have a problem keeping design and code in synchronization. Addressing this issue, this thesis covers my research into this topic and describes an implementation of a tool that speeds up the resynchronization process of design and code. The resynchronization process requires a comparison process between code and design, but such comparison is too complex with too many unsolved problems to fully be covered in just one thesis. As a result, this thesis focuses on UML Class and Sequence diagrams, since they are the most widely and commonly used among the different design notations. Web also restrict the study to just the Java programming language. This thesis highlights the different aspects of the synchronization (bible framework to support people struggling with the ""in-sync"" problem and populates that framework with an initial set of working analysis modules. The thesis then concludes with an analysis of the work and explains how others can extend it to cover more pieces of the problem.",,Carnegie Mellon,Computer Science
2009,Direct Zero-Knowledge Proofs,"A Zero-Knowledge Proof is an interactive protocol which allows one party (the prover) to prove a statement (S) to another party (the verifier) without revealing anything beyond the truth of S. If S is true then the prover should always be able to convince the verifier that the statement is true without revealing anything else. If S is false then the verifier should be able to catch the prover cheating with high probability. For example, Oded Goldreich found a protocol which allowed the prover to convince a verifier that a graph G is k-colorable without revealing anything else. Consequently, all languages in NP are known to have Zero-Knowledge Proofs because they can be reduced to Graph Coloring. However, there are no known direct Zero-Knowledge Proofs for many NP-Complete languages. I present direct Zero-Knowledge Proof protocols for Subset Sum, Clique, SAT and Integer Programming.",Allen Newell Award for Excellence in Undergraduate Research,Carnegie Mellon,Computer Science
2009,Using Machine Learning to Predict Human Brain Activity,"Brain imaging studies are geared towards decoding the way the human brain represents conceptual knowledge. It has been shown that different spatial patterns of neural activation correspond to thinking about different semantic categories of pictures and words. This research is aimed at developing a computational model that predicts functional magnetic resonance imaging (fMRI) neural activation associated with words. The current model has been trained with a combination of data from a text corpus and fMRI data associated with viewing several dozen concrete nouns. Once trained, the model predicts fMRI activation for other nouns in the text corpus with significant accuracy. We hope to explore extensions to the current model in hopes of accurately predicting fMRI activation across subjects and studies. As data is collected with more abstract nouns, new classification techniques will be tested,analyzed and improved. While the current model uses predefined features, this project will explore the usage of various techniques of training and identifying features simultaneously. A survey will also be constructed containing an overview of the machine learning techniques implemented along with the results obtained when used within the model.",,Carnegie Mellon,Computer Science
2009,The Tentacle Arm: Control of a High-DOF Planar Manipulator,"Effective control of a high degree of freedom manipulator increases in computational complexity with the number of joints. A controller for a many-joint arm must include an inverse kinematics solver, a path planner, and a set of object manipulation strategies. I am developing these algorithms for a planar ""tentacle"" arm composed of eight Robotis Dynamixel AX-12 servos in series. The goal is to be able to manipulate objects in real-time. With the algorithms in place, I will demonstrate a variety of operations on objects of different sizes using an actual tentacle arm. The work will be incorporated into the Tekkotsu robot programming framework.",,Carnegie Mellon,Computer Science
2009,Model Checking Cellular Automata,"Simple aspects of the evolution of one-dimensional cellular automata can be captured by the first-order theory of phasespace, which uses one-step evolution as its main predicate. Formulas in this logic can thus be used to express statements such as ""there exists a 3-cycle"" or properties of the global map such as surjectivity. Since this theory has been shown to be decidable by using two-way infinite Buchi automata, it is possible to evaluate these formulas by manipulating the Buchi automata. The goal of this research is to build a system which can evaluate these statements, and to report on the results as well as the tractability of larger problems.",,Carnegie Mellon,Computer Science
2009,Share or Not to Share? The Benefits of the Use of a TabletPC Flash Card Application in an Educational Setting,"Many people use flash cards or index cards to study for vocabulary quizzes, math tests, etc. However, using the physical flash cards is not the best method of studying as people have to maintain the cards and manually sort the cards according to their familiarity. It would be beneficial for students if they could make the flash cards on computers so that they never have to lose the cards and the program could quiz them. The purpose of this study is to extend the Tablet Flash Cards Application that I developed as part of the course 15-397 in fall of 2007 and an independent study in spring of 2008 to a collaborative online learning application. To test the effectiveness of this collaborative online learning application, I will also measure how it can help the eighth grade students at a local middle school in Pittsburgh to learn geometry better. To design and develop this online flash cards application, I will utilize HCI methods such as contextual inquiry, storyboarding, usability analysis, etc.",,Carnegie Mellon,Computer Science
2009,Learning about Related Tasks with Hierarchical Models,"One of the hallmarks of human learning is the ability to apply knowledge learned in previous situations to novel scenarios. In contrast, many machine learning algorithms are explicitly designed to train and test on a single, unchanging distribution of examples. My work is aimed at better understanding how we can extend machine learning algorithms as to enhance their ability to use knowledge from previous experience in novel situations. My project explores two domains where the ability to leverage knowledge learned in previous situations enables performance improvements and new capabilities within a task domain. First, I explore the domain of learning real-world concepts with Bayesian networks. Second, I focus on the domain of signal classification, specifically for detecting seizure events in EEG data recorded from patients with epilepsy. In both of these domains, my approach relies on storing knowledge in a hierarchy with multiple levels of abstraction.",Alumni Award for Undergraduate Excellence,Carnegie Mellon,Computer Science
2009,Exploring Visual Odometry for Mobile Robots,"A key task for an autonomous mobile robot is being able to navigate an unknown environment. An interesting challenge arises when neither a map of the environment, nor the location of the robot are present. In such cases, the robot needs to localize itself and map the environment simultaneously (SLAM). The SLAM problem has been extensively studied using LIDAR as the main sensor. Recently, however, attention is shifting from LIDAR-based SLAM to vision-based SLAM, or vSLAM. Nonetheless, the accumulated unbounded nature of error, due to image noise, prevents vSLAM to scale using visual inputs only. Most of the research in this area integrates other sensors inputs, such as IMU's or wheel odometry to improve navigation scalability. In this work, we focus on the development of a scalable robot navigation system using visual inputs only. Two approaches are proposed: (1) visual odometry, or motion estimates using visual inputs only, to provide accurate geometry estimates within limited distances, and (2) describing the environment as a topological graph-like map to connect the limited segments of visual odometry and allow the system to scale. A series of evaluation and testing experiments will be carried out on video streams from a camera mounted on a mobile robot navigating indoor and outdoor environments.",Alumni Award for Undergraduate Excellence,Carnegie Mellon,Computer Science
2009,Dynamic Path Planning and Traffic Signal Coordination for Emergency Vehicle Routing,"Expedient movement of emergency vehicles to and from the scene of an accident can greatly improve the chance that lives will be saved. One way to shorten the vehicles travel time is through traffic signal preemption which gives emergency vehicles preference at intersections. Early approaches to traffic signal preemption depended on direct signal communication between the vehicle and the intersection. Later, GPS was used to more accurately locate the emergency vehicle. To reduce the emergency vehiclebprogramming framework. s travel time even more, path planning was combined with preemption to allow the vehicle to choose the anticipated least congested route. These previous approaches have two main limitations. First, considering only traffic lights along the emergency vehiclebprogramming framework. s route incorrectly assumes that congestion results only from those lights. Secondly, path planning approaches, while an improvement on traffic signal preemption alone, have adopted a static perspective to route planning, which ignores the possibility that the level of congestion can change during the emergency vehiclebprogramming framework. s journey. This research, therefore, explores two potential enhancements to further reduce emergency vehicle travel time. The first enhancement is a preemption plan that incorporates the traffic lights along the chosen path as well as the lights in the vicinity that might indirectly affect congestion along the chosen path. The second is dynamic path planning using the D*Lite algorithm to dynamically and optimally adjust the chosen path plan based on real-time updates of traffic conditions. The results of this thesis are validated by simulating relevant scenarios using the VISSIM microscopic traffic simulator.",,Carnegie Mellon,Computer Science
2010,Investigating Effectiveness of Small-Scale Lunar Excavators,"This work presents a small-scale (<100kg) lunar excavation rover ""Lysander"", an investigation into its effectiveness for lunar excavation tasks, and a sensitivity analysis of design and operational parameters in point-to-point excavation of small scale lunar excavators. A 2008 Astrobotic study determined that robots with mass of 300kg or less are a viable option for lunar excavation tasks including building protective berms for future lunar outpost landing pads. From this work, a regolith construction simulator dubbed REMOTE was developed to provide a model of the time required to complete lunar excavation tasks for a given platform in addition to a numerical analysis of the sensitivity of design and operational parameters of a rover with respect to its overall task productivity. With the lessons learned from this study, the Lysander rover was developed in 2009 by a small Carnegie Mellon team for lunar excavation research and entrance into the 2009 NASA Regolith Excavation Challenge. The Lysander rover's scraper style excavator design was inspired by the CRATOS rover employing a centrally located scraper bucket for regolith excavation and transport. Using REMOTE, this work models the high and low sensitivity operational and design parameters of the Lysander rover and provides experimental validation of the actual sensitivities. The lessons learned from this analysis present a design methodology showing which parameters to focus on when designing small scale lunar excavators.",,Carnegie Mellon,Computer Science
2010,Temporal Continuity Learning for Deep Belief Networks,"Deep Belief Networks (DBNs) have proven successful for the unsupervised learning of representations of images, and the subsequent use of these representations for object recognition. However, much work remains before DBNs will be able to learn representations as robust as those in Inferotemporal (IT) Cortex, the object recognition center of the human brain. Neurons in IT are selective to particular objects, but the representations are relatively invariant to changes in, for example, object pose or lighting. Physiological evidence suggests that IT neurons learn these representations by assuming Temporal Continuity of visual objects: if an object is present in the visual field at time t, then the same object will most likely be present at time t+1. Thus, if the neural representation changes slowly with time, then the neurons are likely to be responding to objects rather than spurious features. The goal of this research is to show that this constraint, that representations should change slowly with time, can be used train the representations in DBNs, and that the resulting representations will be more useful for object recognition.",,Carnegie Mellon,Computer Science
2010,EyesOn Mobile Eye Tracking,"I hope to deliver a method for creating a low cost mobile eye tracker, with the unique component of a hot swappable scene lens filter. The filter will allow for screen detection in the scene camera using IR LEDs. This, combined with real time eye-tracking software, will allow the mobile eye-tracker to be used as a computer input device. I also plan on providing a rigorous analysis of the accuracy of the device in several different lighting settings, a portion of data distinctly absent from other open source eye-tracking solutions. This will allow real time eye-tracking as a method of computer input on a head mounted mobile eye-tracker, and create the opportunity for quality research at a low, affordable cost.",Alumni Award for Undergraduate Excellence,Carnegie Mellon,Computer Science
2010,Complementing Buchi Automata Over Bi-infinite Words,"In this work, we introduce an algorithm that, when given an Ï-BuÂ¨chi automaton (an automaton recognizing languages over one-way infinite words) as input, finds a small automaton that recognizes the complement of the language of that automaton. This procedure is necessary in many tasks, and the current algorithms construct automata much larger than the original. We modify the definition of an Ï-BuÂ¨chi automaton to allow a smaller machine to represent the same language, allowing the complement automata to be smaller as well. We then show that a natural generalization of an existing algorithm for complementing Ï-BuÂ¨chi automata will find the complement of the language of one of these reduced automata. The modification we make to the definition of an automaton is to allow transitions to be labeled with arbitrarily long strings of input characters, rather than restricting them to reading one character at a time. This causes some of the states to become extraneous, which allows us to remove them without changing the language accepted by the automaton. We can iteratively find and remove extraneous nodes, and add transitions between the removed node's neighbors. We explore different possibilities for the ordering in which to remove the nodes that produces the smallest resulting automaton, though the problem in general is NP-complete. This modified version does have some drawbacks, but in some applications of this algorithm, none of these drawbacks are relevant. We focus on one such application - deciding properties of cellular automata that can be described with a formula of first-order logic containing a single universal quantifier. These properties can be verified with operations on a Î¾-BuÂ¨chi automaton (an automaton recognizing languages over bi-infinite words) on a de Bruijn graph, but the presence of a universal quantifier requires taking the complement of the language, causing a superexponential blowup in the number of states required. We show that by converting the Î¾-BuÂ¨chi automaton to a reduced Ï-BuÂ¨chi before performing this step, we can greatly reduce the number of states in the complement automaton.",,Carnegie Mellon,Computer Science
2010,Using Machine Learning Techniques to Uncover What Makes Understanding Spoken Chinese Difficult for Non-native Speakers,"The Chinese dictation tutor has been used for the past few years in over thirty classrooms at universities around the world. A large amount of data have been collected from this program on the types of errors students make when trying to spell the pinyin of the Chinese phrase spoken to them. I plan to use this data to help answer the question of what is hard about understanding Chinese. Is it a particular set of consonants, vowels, or tones? Or perhaps do certain difficulties arise in the context in which these sounds are spoken? Since each pinyin phrase can be broken down into features (consonants, vowel sounds, and tones), we can apply machine learning techniques to uncover the most confounding aspects for beginning students of Chinese. We can extend the methods we developed here to create an ML engine that learns on the fly for each student what they find difficult. The items to be presented to the learner can be chosen from a pool that has features with the lowest probability of being correctly classified by the student. This will allow the Chinese learner to focus on what he or she is having most difficulty and hopefully more quickly understand spoken Chinese than without such focused ""intelligent"" instruction.",,Carnegie Mellon,Computer Science
2010,Learning-based Change Detection for Mobile Robots,"Mobile Robotics has advanced to a stage where robotic vehicles are beginning to navigate autonomously and make decisions about the traversibility of obstacles based on rich sensor data and machine learning and planning techniques. Unfortunately, these systems can still fail, especially in circumstances on which they were not trained. My goal is to create a technique which should improve robot safety and reliability by allowing robots to detect important changes in their environment. If a robot sees a given scene more than once and there is a significant change, such as a human being present or a tree falling, the robot should be able to detect the change and avoid it or alert a human. I am investigating machine learning techniques to perform change detection on a set of rich sensor data collected by a mobile robot.",Allen Newell Award for Excellence in Undergraduate Research,Carnegie Mellon,Computer Science
2010,Enhancing Audio Reproduction through Improved Instrument Synthesis Models for Violin and String Instruments,"Current methods of psycho-acoustic based audio compression work very well, but at a cost of loss of original information. Simultaneously, loss less methods of compression on average compress at approximately a 2:1 ratio. One means of improving the quality of sound reproduction while reducing the amount of information needed to encode the sound is through improved sound synthesis techniques. In this research, we focus on using the combined SIS method to reduce the amount of information needed in a control channel for a violin synthesizer, while maintaining sound quality at levels that would make it almost indistinguishable from an actual instrument being played. SIS has found success in brass and woodwind instruments, but has encountered challenges in synthesizing string instruments. Through the introduction of additional parameters, we hope to reduce the amount of control information needed to express sounds on those instruments.",,Carnegie Mellon,Computer Science
2010,Layperson-trained Speech Recognition for Resource Scarce Languages,"We develop practical methods in speech recognition for low-resource languages overlooked by most effort in the field today. The resulting technique will potentially provide speech technology for languages of the developing world where many of the speakers are illiterate, by allowing one to build low-cost speech recognizers with high accuracy over small vocabularies involving minimal audio data and human expertise for training. Specifically, we will design pronunciation-generating algorithms for new languages based on existing speech recognition engines for other languages through cross-language phoneme mapping. We will also develop methods that will eliminate the need for language expert-based training.",,Carnegie Mellon,Computer Science
2010,Finding Accurate Models of Social Graphs,"Studies into the graph structure of social networks have revealed many interesting properties, including their tendency towards clustering and the small world eo,ect. The original random graph model proposed by Erdos and Renyi does not exhibit these properties, prompting investigation into other algorithms to construct random graphs. Using large data sets, such as the 65 million node data set recently collected from Twitter. I determine what properties actual social network graphs have that the random graph algorithms do not accurately model, and present methods of producing random graphs that correctly model those properties.",,Carnegie Mellon,Computer Science
2010,Human-powered Word Sense Disambiguation,"One formidable problem in language technology is disambiguating the ""sense"" of a word as it occurs in a sentence. For example, distinguishing ""bank"" as a river bank or as a financial institution. This work explores a specific strategy for solving this problem; the strategy involves harnessing the semantic abilities of human beings to develop datasets that can be used to train machine learning algorithms.",,Carnegie Mellon,Computer Science
2010,Predicting Risk from Financial Reports with Supervised Topic Models,"Forecasting from analysis of text corpora is an exciting research area, one that has potential for application to a variety of fields such as finance, medicine and consumer research. We apply techniques from NLP to predicting real-world continuous quantities associated with a forward-looking text's meaning. In particular, we study Financial Reports because of the presence of a large text corpus that is highly standardized and widely studied by financial analysts in industry. In conducting our analysis we use a class of generative probabilistic models known as Topic Models. In such a model, documents are a mixture of topics, where a topic is defined as a probability distribution over words. These models are interesting because they provide a simple probabilistic procedure for generating documents. Such a procedure can be inverted using standard statistical techiques, allowing us to infer a set of topics from which a particular document was generated. The problem then is to associate the inferred topic distributions with real-world quantities such as company-level financial indicators for the prediction task.",,Carnegie Mellon,Computer Science
2010,Inductive Inference of Integer Sequences,"Inductive inference denotes hypothesizing a general rule from examples. In this senior thesis, we give a model for inductively inferring integer sequences. Within this model we give algorithms that can (inductively) infer integer sequences with high confidence, under the assumption that all the terms of the sequence are accessible. We provide tight bounds on the number of sequence terms an inference algorithm needs to see before it can make an inference it is confident in. We use The On-Line Encyclopedia of Integer Sequences (OEIS) to evaluate our model. We are currently able to infer about 18.9% of the OEIS using the methods given in this thesis. We also implement a website for inferring sequences that is meant to complement the OEIS. This site can be publicly accessed at http://atemi.cdm.cs.cmu.edu/~samt/sequences.html",,Carnegie Mellon,Computer Science
2010,Local and Global Perspectives: An Investigation of How Cultural Factors Contribute to Gender Balanced Participation in Computer Science,"This thesis is based on the premise that cultural factors play an important role in the representation of women in computer science. I will argue that gender differences, usually noted as a primary source of women's low representation, do not provide a satisfactory explanation for the low participation of women in computer science (CS) and that we need to look at factors other than gender differences. This thesis will begin by exploring research which shows that the underrepresentation of women in computer science (well documentated in the United States) is not a universal phenomenon. Some countries such as Bulgaria are graduating women in engineering degrees in numbers equal to those of men. I plan to investigate the cultural factors that make this possible. Additionally, I plan to investigate the local culture at Carnegie Mellon where we enroll women in computer science at twice the average to other research 1 universities to investigate the local cultural factors at play. From this I hope to be able to make recommendations to engage a more diversified population in computer science in the United States.",,Carnegie Mellon,Computer Science
2010,Inference of Population Structure with Optimal Number of Ancestral Groups,"In this project I study the problem of population structure inference using multi-locus genotype data. Traditional methods for inferring population structure such as Structure program or mStruct does not present a good way to optimize the number of ancestral population groups by including the number in the model and inferring from the model itself. Therefore I aim to present a model that will have the ability to infer the optimal number intrinsically. So far studying the previous works, implementing parts of them, and coming up with new model have been done. For the rest of the semester I plan on finishing implementations including the new model and coming up with an inference method for the model.",,Carnegie Mellon,Computer Science
2010,Rich Named Entity Recognition,"There is a need to identify named entities in free text, including references to people, organizations, and locations. The standard datasets for building named entity recognizers are limited to these three coarse-grained category types, but I am interested in a larger set of more specific labels (e.g., languages, geopolitical entities, nationalities, diseases). There are proprietary tools for named entity recognition that provide these labels (but with some level of error). The goal of this project is to build a named entity recognizer that provides rich labels, using a large unlabeled dataset and the output of the proprietary tool. This will involve a little bit of machine learning and parallel processing using cloud computing also.",,Carnegie Mellon,Computer Science
2010,Education e-Village: Empowering Technology Educators in Developing Regions,"There exists a significant need for relevant, accessible and useful resources to enhance technology education in developing regions. Currently, access to technology courseware is available via several online resources. These are designed for developed communities, where technology resources are ubiquitous. There are no avenues for understanding the courseware or discussing alternative ways of teaching it based on constraints specific to developing regions, like limited resources and experiences with technology. This prompted the conception of TechBridgeWorld's ""Education e-Village"" (E-Village) project to address the aforementioned needs. E-Village is an online community where educators from around the world will be able to share ideas, experiences, expertise, educational resources, and strategies for effective technology education in developing regions. This work is involved with 4 critical aspects of E-Village namely, search functionality, discussion board functionality, a learning tool, and user experience. The aims of this work are threefold. First, it will design and implement initial versions of the outlined features keeping existing constraints and conventions in mind. Second, it will understand the effectiveness of the implementations through feature-specific usability tests, and address any major shortcomings. Finally, it will perform overall usability tests, present key findings from these tests and make recommendations for enhancements in future.",,Carnegie Mellon,Computer Science
2010,Designing Mobile-phone Based Educational Games to Improve the English Literacy Skills of Limited English Proficient (LEP) Adults,"Many immigrant workers in the fields of construction, menial labor, household help, and service sectors in countries like the US and Qatar have limited English proficiency, which is often a barrier to advancement in their careers and creates problems in communication with their supervisors and managers, which in turn leads to a myriad of other problems. These problems also expand to the refugee population, who find it hard to get jobs with limited English skills, thereby making their adjustment process in the US harder. English proficiency is also a problem with the deaf population; only 10% of the 18-year olds read at or above an 8th grade level, and the average deaf adult reaches only a 4th grade reading level. We can address the problems of limited English proficiency by providing these groups with a fun way of practicing their English skills. The senior thesis is motivated by the aforementioned problems faced by immigrant adults, refugees and deaf students and proposes to develop a mobile-phone based educational game for enhancing the English communication skills of these limited English proficient (LEP) adults. The senior thesis will build on mobile phone based literacy tools developed during TechBridgeWorld's iSTEP program in 2009, which consists of two parts: the actual single player game accessible via mobile phones, and an online content authoring system to allow teachers or other informed individuals to add content to the games. So far, a majority of the related research efforts have concentrated on the effectiveness of similar educational games on children, and the thesis will contribute to the understanding the effectiveness of mobile phone based educational games on adult literacy. This will require significant research to effectively incorporate techniques used to teach English as a Foreign Language (EFL) to adults into educational games that are exciting to play, effective in teaching, and accessible via a mobile phone.",,Carnegie Mellon,Computer Science
2011,Discriminative Pronunciation Learning for Speech Recognition for Resource Scarce Languages,"We develop pragmatic solutions that create small vocabulary speech recognizers at a fraction of the cost and time that current packages require. Little to no expertise in speech recognition is needed in training a recognizer for any language, and only a few audio samples per word are required. Hence, these solutions are ideal for targeting languages that have a small or economically disadvantaged user base which are typically ignored by the commercial world. In particular, building on previous work, we design algorithms that utilize current speech recognition engines to generate pronunciations which map phonemes across languages. Using discriminative training techniques, we further improve the pronunciation generated, resulting in higher recognition accuracy.",,Carnegie Mellon,Computer Science
2011,Bilingual Part of Speech Tag Induction with Markov Random Fields,"This paper explores unsupervised learning with undirected graphical models. We focus on the problem of bilingual part of speech (POS) induction, which considers the POS induction problem when parallel training data is available [Snyder et al., 2008]. Because we use undirected models, there are no restrictions on the structure of the graphs and we can incorporate many overlapping features, such as sublexical features (this is in contrast to previous work which made use of directed, generative models). Although our undirected model is quite flexible in terms of being able to add new features, the unsupervised learning problem turns out to be quite challenging, and analysis determines that the non-convex objective we are attempting to optimize has many local optima which causes problems for learning. We show that performance can be improved by using an alternative objective based on contrastive estimation [Smith and Eisner, 2005b].",,Carnegie Mellon,Computer Science
2011,Generating Giant Word Corpora with Human Computation to Solve Word Sense Disambiguation,"Given a word within a sentence, how can a computer determine the meaning of that word? If there is only one given definition of the word, the solution can be easily determined. If multiple definitions exist however, this problem becomes magnitudes more difficult. The open problem of Word Sense Disambiguation, hereafter referred to as WSD, has yet to be adequately solved. The applicability of machine learning to this problem is obvious. However, the major issue with such an approach is a lack of data with which we can train a machine learning algorithm. Our solution to this issue is to apply Human Computation. We will create a game that will generate a giant corpus of (word, sentence) pairs tagged with a disambiguated definition for that word from previously untagged sentences. Using this corpus, we will gain the ability to successfully train a machine learning algorithm to be robust enough to solve WSD on a domain as large as an entire language. Thus, an effective solution to WSD can be created.",,Carnegie Mellon,Computer Science
2011,Slick: A Framework for High Throughput Network Applications in the Kernel,"With the increasing use of the Internet and networked services, the ability to make such services perform better, speci cally in terms of increasing throughput and, by extension, the number of requests that can be handled, is more important than ever. Despite many interface and architecture improvements, such services are constrained by the maintenance of the process abstraction (i.e. the isolation of tasks from each other and the inner workings of the operating system), which imposes a great cost on every network transaction. Slick tackles this problem by providing a convenient interface for providing these services in the kernel. In this work we explore the creation of this framework and the performance implications of its use.",,Carnegie Mellon,Computer Science
2011,An Efficient Algorithm for Nonparametric Online Prediction,"Online learning processes input data as a stream, predicting a label for each incoming instance based on the observations in the history. Most of the current approaches to online learning focus on a single learning task. In this senior thesis research, we will study problems of multiple parallel online predictions under computation constraints. Our goal is to find approaches to model the prediction loss and cost and also, by carefully allocating the resources, to minimize the overall loss across tasks with fixed total cost.",,Carnegie Mellon,Computer Science
2011,Dynamic Casts in the Plaid Programming Language,"Typestate is a new paradigm in programming language design that allows programmers to explicitly specify state transitions, which include the addition and removal of the fields and methods of objects at runtime, within their programs. Plaid, a general-purpose programming language in development at Carnegie Mellon, reifies this idea of typestate in an actual implementation. In Plaid, object aliasing complicates the static verifcation of state transitions by making it impossible to know with certainty the state of all other objects after a transition has been performed [1]. Plaid solves this issue with permission kinds, which help programmers as well as the compiler reason about aliasing in programs. In most languages, runtime or dynamic casts must be introduced either explicitly by the programmer or implicitly by the compiler at certain points in a program in order to ensure that the language is typesafe. The addition of aliasing information to a gradual type system raises several issues in the implementation of these casts. In order to cast something to a type with a speci c permission, aliasing information must be maintained at runtime to verify that the resulting permission is compatible with all other existing permissions for that object. For my thesis I defined a static and dynamic semantics for dynamic casts in the Plaid programming language, incorporated these semantics into the Plaid compiler implementation, and examined the impact of this implementation on the overall performance of compiled Plaid programs.",,Carnegie Mellon,Computer Science
2011,Human-Like Understanding of Two-Line Figures,"We present a computational theory of mid-level vision within the Line Pair micro-domain, implemented in Mathematica. The theory assumes that categorization is the fundamental aspect of understanding, and that binding, symmetry, regularity detection, and proportionality detection are the mechanisms by which we categorize instances. We discuss additional aspects of ""understanding"" the domain, and suggest other domains to which the theory might transfer.",,Carnegie Mellon,Computer Science
2011,CaFE Play: A Customizable Mobile Phone Game for Enhancing English Literacy,"English proficiency is an invaluable skill in many parts of the world. It often helps individuals improve their quality of life and contribute more to society. However, for various reasons, many individuals have difficulty learning English, often simply due to lack of guided practice. To help address this issue, the TechBridgeWorld research group (www.techbridgeworld.org) through their TechCaFE program (http://www.techbridgeworld.org/techcafe/) has developed the CaFE Phone tool: a prototype mobile phone-based game for enhancing English literacy. The purpose of this thesis is to improve and build upon the game by creating a game customization framework, CaFE Play, which can be used to not only customize the existing game, but also to create other such customizable games. These games should increase the user's motivation to practice English literacy, and thereby better improve the user's English proficiency, a hypothesis that this thesis also aims to test.",,Carnegie Mellon,Computer Science
2011,Online Metric Matching on the Line,"Given a metric space, a set of points with distances satisfying the triangle inequality, a sequence of requests arrive in an online manner. Each request must be irrevocably assigned to a unique server before future requests are seen. The goal is to minimize the sum of the distances between the requests and the servers to which they are matched. We study this problem under the framework of competitive analysis. We give two O(log k)-competitive randomized algorithms, where k is the number of servers. These improve on the best previously known O(log2 k)-competitive algorithm for this problem. Our technique is to embed the line into a distribution of trees in a distance-preserving fashion, and give algorithms that solve the problem on these trees. Our results are focused on settings for the line, but these results can also be extended to all constant-dimensional metric spaces.",,Carnegie Mellon,Computer Science
2011,Market-Based Coordination of Recharging Robots,"Autonomous recharging is becoming increasingly important to mobile robotics as it has the potential to greatly enhance the operational time and capability of robots. Existing approaches, however, are greedy in nature and have little to no coordination between robots. This leads to less efficient interactions which adversely affect the performance of the team of robots. Therefore, improved coordination can greatly enhance the performance of such a team. This senior thesis has advanced the state of the art in autonomous recharging by developing, implementing, testing, and evaluating a market-based distributed algorithm for effectively coordinating recharging robots. This system is charge-aware and able to autonomously account for current and future battery states as well as current and future tasks. The developed solution has been evaluated on a series of tasks run on worker robots and recharger robots. Simulations were also used to validate the system on larger workloads. Results show that our approach consistently outperforms the state of the art in recharging strategies.",,Carnegie Mellon,Computer Science
2011,Decision Problems on Iterated Length-Preserving Transducers,"Finite-state transducers are simple theoretical machines that are useful in expressing easilycomputable functions and relations. This investigation considers the relations formed when transducers are iterated arbitrarily many times, a construction which is useful in model checking. In particular, we consider a number of decision problems over various classes of transducers, and attempt to determine whether each decision problem is decidable. For example, a Turing machine construction can show that the string reachability problem on arbitrary iterated transducers reduces from the halting problem, and is thus undecidable. This setup is useful for investigating how restricted a transducer must be before its iteration is no longer able to simulate a Turing machine (for di erent notions of simulation). The main result of the paper is that both reset transducers|which have a highly restricted concept of memory|and binary toggle transducers|which must express all letter transformations as permutations|are capable of simulating a Turing machine computation.",Allen Newell Award for Excellence in Undergraduate Research,Carnegie Mellon,Computer Science
2011,Encoding Natural Priors in Neural Populations,"Bayesian theories of the brain have provided insights into perception, but the underlying neural mechanisms which could implement these computations remains unknown. To perform Bayesian inference, sensory information must be combined with prior information about the natural world. We investigated how these natural priors could be learned and encoded in populations of neurons in primary visual cortex. We found that the distribution of neuronal tuning properties for depth-tuned neurons was very similar to the distribution of depths occurring in natural scenes. This finding is consistent with the hypothesis that neurons are performing optimal sampling of the natural environment based on the information maximization principle. By using the priors encoded in the tuning properties of neuronal populations, we were able to develop a framework for performing Bayesian inference in the brain.",,Carnegie Mellon,Computer Science
2011,The Design and Implementation of a Power-Aware Load Balancer,"Energy costs for data centers are doubling every five years and have already crossed $19 billion. However, much of this power is wasted as servers are mostly idle. Idle servers can also consume as much as 60% of peak power consumption. We introduce a power management algorithm called AutoScale which reduces power consumption by over 30% while delivering response times that are only slightly longer, and still meet service level agreements. AutoScale works by dynamically provisioning data center capacity as needed. AutoScale is load-oblivious and can also be deployed as a distributed application. It is also computationally cheap. We evaluate AutoScale in a testbed structured as a multi-tier data center. A new benchmark is developed to test jobs which work by performing key-value requests upon a data storage. Emphasis is given on testing by implementation rather than by simulation and on comparison against existing power management techniques.",Alumni Award for Undergraduate Excellence,Carnegie Mellon,Computer Science
2011,Learning Classifiers from a Relational Database of Tutor Logs,"A bottleneck in mining tutor data is mapping heterogeneous event streams to feature vectors with which to train and test classifiers. To bypass the labor-intensive process of feature engineering, AutoCord learns classifiers directly from a relational database of events logged by a tutor. It searches through a space of classifiers represented as database queries, using a small set of heuristic operators. We show how AutoCord learns a classifier to predict whether a child will finish reading a story in Project LISTENs Reading Tutor. We compare it to a previously reported classifier that uses hand-engineered features. AutoCord has the potential to learn classifiers with less effort and greater accuracy.",,Carnegie Mellon,Computer Science
2011,Mobile Cloud Computing for Data-Intensive Applications,"The computational and storage capabilities of today's mobile devices are rapidly catching up with those of our traditional desktop computers and servers. In fact, mobile phones with 1 GHz processors are readily available in the market today. Unfortunately, all of these processing resources are mostly under-utilised and are generally used to process local data and programs only. With the use of local wireless networks, we can enable these phones to communicate with each other without utilising the resources of a global cellular network. This has the potential to enable collaborative data-intensive computing across a cloud of mobile devices without straining the bandwidth of global networks. To achieve these objectives, Hyrax [3] was initially developed by Marinelli as a MapReduce system [1] that is deployed on a networked collection of Android smartphones. Hyrax is based on Hadoop [4], which is a Java implementation of the MapReduce system and the Google File System [2]. While Marinelli has developed a system that is suitable for initial use to discover the resource constraints/challenges, performance and the scalability aspects of using mobile devices for collaborative data-intensive computing, that initial implementation of Hyrax was not suitable for wide-scale deployment on the mobile devices of common users. To that end, we have improved on Marinelli's implementation of Hyrax, and aim to develop a mobile multimedia share-and-search application that allows users to discover relevant multimedia content on the mobile phones of those within reasonable proximity (ie within the same wi network). We also evaluate the performance of this new implementation of Hyrax and identify areas for future work, especially in improving the performance and resource consumption (especially power resources) of Hyrax. Furthermore, we also consider the risk that the use of Hyrax poses to the users of the mobile devices that run Hyrax.",,Carnegie Mellon,Computer Science
2011,Kernel Accommodations for I/O Intensive Workloads,"Recent advances in storage technologies have lead to widespread availability of solid state storage devices. These devices are extremely different from their machanical counterparts in that they can process I/O commands at a far superior rate. Suboptimal performance of nodes in the FAWN (Fast Array of Wimpy Nodes) project inspired this work, where Intel Atom processors could not saturate the random read rate of the solid state drive like an Intel X25-E. We probe the command rates that can be achieved by Linux drivers and the kernel's I/O stack.",,Carnegie Mellon,Computer Science
2011,Place Recognition for Indoor Blind Navigation,"Navigating inside buildings is a challenging task for the blind which often makes them dependent on others for guidance. This thesis explores place recognition and navigation algorithms accessible via hand held devices to guide the visually impaired inside unfamiliar buildings. In computer vision, place recognition addresses the problem of identifying places based on appearance. The majority of this work focuses on implementing, evaluating, and enhancing place recognition algorithms for this application domain. The outcome of this work is a camera-based indoor blind-navigation model that can autonomously recognize previously mapped indoor locations. In the mapping or calibration phase, images of an indoor setting are collected, labeled, and mapped to specific locations in the indoor environment. The effectiveness of the system is evaluated by measuring the recall rate of the place recognition algorithm.",,Carnegie Mellon,Computer Science
2012,Temporal Analysis of Information Cascades on Twitter,"There has been much work in the area of information propagation. However, most of this work has been node-centric or edge-centric. My thesis is taking a more global approach, looking at properties of the entire graph over time. Specifically, I am analyzing how the closeness centrality of the spread of hashtags changes over time within Twitter.",,Carnegie Mellon,Computer Science
2012,A Type Theory of Linking,"The processes of linking and loading, while fundamental to the way code is actually executed on modern computers, have hitherto evaded type-theoretic explanation. This prevents us from producing a typesafe linker and loader, which would satisfactorily close the gap between high-level typesafe programming languages and low-level typed assembly languages and proof-carrying code. We develop such a type theory, by way of defining the syntax and semantics of a lambda-calculus with explicit support for libraries. As a proof of concept, we implement a compiler from this calculus to a simple abstract machine language and associated object-file format, and an interpreter for this machine.",,Carnegie Mellon,Computer Science
2012,ColorMyGraph: Student Proof Analysis and Verification,"College-level Mathematics and Theoretical Computer Science is almost universally based in proof. We prove our theorems true, our code correct, our invariants constant. Traditionally, students struggle a lot with learning the art and science of writing proofs. Often, it presents such a barrier that they give up on understanding what to do. My senior thesis focuses on the development, implementation, and testing of a new type of assignment which we call a ""Verification Assignment"" in which students review and comment on each others' proofs via an online system. Our hope is that these assignments simultaneously help students gain a better understanding of the underlying concepts as well as decrease the time spent on grading.",,Carnegie Mellon,Computer Science
2012,Applications of Spectral Algorithms,Spectral algorithms exploit information on the graph spectrum to gain computational speedups. The goal of this thesis is to unify various classical graph algorithms into a common spectral graph framework. We would show an application of spectral graph algorithms where we segment nerve fiber layers from optical coherence tomography images. The key advantage of the unified spectral application approach is that the different algorithms can share common data structures and sub routines.,Allen Newell Award for Excellence in Undergraduate Research,Carnegie Mellon,Computer Science
2012,Transparent System Call Based Performance Debugging for Cloud Computing,"Problem Diagnosis and debugging in concurrent environments such as the cloud and popular distributed systems frameworks has been a traditionally hard problem. We explore an evaluation of a novel way of debugging distributed systems frameworks by using system calls. We focus on Google's MapReduce framework, which enables distributed, data-intensive, parallel applications by decomposing a massive job into smaller (Map and Reduce) tasks and a massive data-set into smaller partitions, such that each task processes a different partition in parallel. Performance problems in such systems can be hard to diagnose and to localize to a specific node or a set of nodes. Additionally, most debugging systems often rely on forms of instrumentation and signatures that sometimes cannot truthfully represent the state of the system (logs or application traces for example). We focus on evaluating the performance of the debugging these frameworks using the lowest level of abstraction - system calls. By focusing on a small set of system calls, we try to extrapolate meaningful information on the control flow and state of the framework, providing accurate and meaningful debugging.",Alumni Award for Undergraduate Excellence,Carnegie Mellon,Computer Science
2012,Automatic Heap Exploit Generation,"The automatic exploit generation (AEG) challenge is, given a program, automatically find vulnerabilities and generate exploits for them. Avgerinos et al showed that, given the source code of the program, AEG was possible for certain stack smashing and format string exploits. In Automatic Heap Exploit Generation (AHEG), we do away with the need for source code and we extend AEG to automatically find heap bugs and generate heap exploits on applications running on Windows XP SP3. In this semester, we developed the techniques and tools for automatically discovering heap bugs. Next semester, we will automate exploit generation.",,Carnegie Mellon,Computer Science
2012,Mode Checking for Lazy Functional Logic Programming,"Functional logical programming is a paradigm that introduces predicate satisfaction as a first class construct into the functional setting. Given the difficulty of reasoning about the time complexity of logical code, lazy evaluation is desirable. In consolidating logical and functional semantics, a complete logical query primitive is necessary. In the non strict and breadth first evaluation strategy, it is possible to return unground logical variables. Such results are meaningless in the context of determinism. Given the semantics for a practical lazy functional logical language, we attempt to supply a flexible type and mode system in order to ensure progress.",,Carnegie Mellon,Computer Science
2012,Word Problem Solving using Sequence Inference,"Currently, word problems can be understood by humans but cannot be easily understood by computers. This project proposes using sequence inference as a bridge to this problem. Instead of attempting Natural Language Processing, which is difficult, a human observes a parameter in the problem, and records the answers to the question for small values of the parameter. This is often easy for the human to do but nonetheless provides an effective description of the problem to the computer. The computer extrapolates the sequence given to provide insights about the problem, which it reports back to the human, allowing the computer to participate in informal problem-solving with a human.",,Carnegie Mellon,Computer Science
2012,Feature Detection on an FPGA,"Fast, accurate, autonomous robot navigation is essential for planetary robots when communication with Earth is delayed, limited or nonexistent. Feature detection is the crux of robot autonomy. It calculates feature descriptors which are the basis of visual odometry and obstacle avoidance. The first step in calculating a motion estimate using visual odometry is to detect features in a pair of stereo camera images mounted on a vehicle. Without feature detection, the robot cannot determine where it is and how to get to its destination. Feature detection is understood, but requires fast computation, which is not available on planetary rovers. The feature slows rover progress. Consequently, the feature is turned off, risk is accepted, and exploration progress is possible. For example, the Mars Exploration Rover was entrapped and lost when feature detection was turned off. The need is for fast, spaceworthy feature detection. This means near-real-time speed, low power consumption, and radiation tolerance. This research pro- poses to accomplish this vision by implementing feature detection on a Field Programmable Gate Array (FPGA). FPGAs possess the physical attributes for success. The question is whether a feature detection algorithm can be manifested on an FPGA.",,Carnegie Mellon,Computer Science
2012,Graphical Numerical Inference: a.k.a. Brain Surgery for Excel,"Excel's drag and auto-fill feature works for most simple numerical cases like addition. However, it fails when someone gives it a checkerboard pattern with 1s and 0s and tries to extend the pattern. Excel is unable to expand this obvious pattern because its entire inference is based on a static snapshot of the final data. Graphical Numerical Inference Program (GNIP), on the other hand, takes a dynamic approach by monitoring how the sequence is being filled. It will then try to figure out how the user is filling up the entries. After that, it picks up where the user has left off and fills in the rest of the entries.",,Carnegie Mellon,Computer Science
2012,Leveraging Social Networking and Indoor Localization for Context-Aware Ubiquitous Systems,"The proliferation of social-networks along with technologies allowing for better realization of pervasive systems, facilitate the construction of social-based context-aware systems. We leverage this advancement to create a generic and novel social context-aware ubiquitous system, SCOUT, above which a multitude of soft real time mobile applications can be built. We propose a detailed design of SCOUT, implement a prototype as a proof-of-concept and evaluate the responsiveness of the system. Among various challenges in the domain of social-based context-aware systems, we address the challenge of maintaining end-to-end communication channels between ubiquitous systems and mobile users. Mobile users are often situated in heterogeneous networking environments that provide intermittent, highly interfering, or highly disruptive communication. In such situations, users either resort to more stable but charged internet services (like 3G) or remain disconnected. Hence, this reduces the effectiveness of ubiquitous systems in both gathering and disseminating information to and from mobile devices in soft real time systems. We propose a hybrid system architecture that bridges the centralized and distributed paradigms building upon opportunistic and disruption tolerant networking research. Based on this architecture, we introduce a means for a novel mechanism for information seeking users. Finally, we use our ubiquitous system, SCOUT, as a platform to implement and evaluate the effectiveness of our architecture using three metrics namely: success rate, delay and cost.",,Carnegie Mellon,Computer Science
2013,Leveraging Dependency Parse Structures for Large-Scale Noun Phrase Classification,"How do you construct a program that can read the web and learn about the world? How could a program process billions of sentences, learn the semantic meaning of hundreds of thousands of noun phrases, and use this knowledge to classify unobserved noun phrases? We developed a novel three-stage algorithm for noun phrase classification and present empirical results on a 3 TB corpus of parsed English text. Each sentence is represented as a directed, acyclic graph, where tokens are vertices and labeled edges represent the syntactic dependency relationship between tokens. The algorithm's first stage learns graph walk strategies for associating semantically related noun phrases. The second stage uses these strategies to build a rich noun phrase feature space. The third stage learns a logistic regression model on this constructed feature space that is highly effective at noun phrase classification. We present this novel three-stage algorithm in detail and report its performance on several noun phrase classification experiments.",,Carnegie Mellon,Computer Science
2013,Predicting an individual's brain state induced by music listening,"Our general goal is to establish a relationship between the features of a musical piece and the features of the neural activities induced by listening to a piece of music. This work provides evidence that this relationship exists and can be modelled. Furthermore, it provides evidence that a model learned on a number of song brain state pairs generalizes to predict the neural activities corresponding to unheard pieces of music. Modelling this relationship will allow us to create novel applications in music information retrieval and music therapy. Music information retrieval applications include the retrieval or synthesis of a piece of music to suit an individual's preference or desired cognitive state. One music therapy application is the retrieval of pieces of music that aid in priming the brain for particular tasks. To this end, in this study we first recorded brain activities of multiple subjects through electroencephalography (EEG) while they repeatedly listened to two different pieces of music. Classification of the musical piece that the subject was listening to was performed in order to ensure that listening to a song induces repeatable brain states. After this successful classification we ran another experiment in which a subject repeatedly listened to music from two classes. Within each class the songs are similar to each other in the music feature space? across classes they are dissimilar. In order to ensure our model can generalize to unheard pieces of music classification was performed with a cross validation scheme that consisted of training on the EEG examples corresponding to a fraction of the songs in each set and then classifying the examples corresponding to the remaining songs. We achieved classification accuracy above 90%. The success of this classification clearly shows that similar music induces similar brain states, and thus exhibits the feasibility of our general goal.",,Carnegie Mellon,Computer Science
2013,Evaluation of Loop Nesting Forest Algorithms,"Advanced control-flow analyses are often built on a technique known as structural analysis (SA), which iteratively transforms a control-flow graph (CFG) into a control-flow tree with leaves corresponding to CFG nodes and internal nodes corresponding to CFG regions. An iteration in SA typically proceeds by first identifying a region in the CFG that matches one of the known high-level control structures such as if-then-else and do-while, then contracting the region into a new node. In practice, however, many CFGs contain large regions that do not match any high-level control structures (e.g., due to the use of break), thus making control-flow analysis less precise. This is why researchers have recently started complementing SA with iterative refinement, which is the idea of using iteratively removing of a carefully-chosen arc to allow SA to continue and discover the finer structures embedded inside such regions. Unfortunately, their experience shows that straightforward implementations of SA with iterative refinement tend to be bug-ridden. In particular, the addition of refinements substantially complicates the control flow of the SA algorithm itself. Our insight is that we can partition the arcs removed during refinement into two groups depending on whether an arc is a backward feedback arc or not. (A feedback arc is an arc whose removal breaks a cycle, and the arcs removed by refinement are all backward.) This naturally leads us to a new, conceptually-clean implementation strategy by handling cyclic and acyclic regions in two independent phases. The first phase preprocesses a CFG into a so-called ""loop-nesting forest"" (LNF) using existing loop identification algorithms in the compiler literature. The result is a hierarchical decomposition of the CFG into acyclic regions, with all candidate feedback arcs identified. The second phase applies SA in each of the acyclic regions, which can be implemented much easier since each region is guaranteed to be acyclic. I propose to extend the CMU Binary Analysis Platform (BAP) with LNF algorithms from the literature. This will enable us to evaluate the effectiveness of the aforementioned two-phase structural analysis algorithms versus the existing mixed implementation in BAP.",,Carnegie Mellon,Computer Science
2013,Orbits of Iterated Binary Transducers,"This project looks at a special case of transducers, specifically binary invertible transducers. Starting with simple 3 state transducers and moving to more complex ones, we address questions about the orbits of these transducers. We try to provide a general ways to find if two elements are in the same orbit. Categorizing the transducers with efficient solutions to these problems could open applications in program verification where we want to check if a program ever enters a ""bad"" state.",,Carnegie Mellon,Computer Science
2013,Classification and Automaticity of Discrete Dynamical Systems,"Model checking is a method of building a model of a system and then exhaustively and automatically checking whether that model meets a given specification. In particular, the system could require infinitary descriptions (e.g., runs of a reactive system). Elementary cellular automata (ECA) form simple dynamical systems that exhibit a wide array of behaviors, ranging from trivial to computationally complete. I've built the automata-theoretic support structure to perform model checking on first-order logic formulae on one-way infinite cellular automata. Using these formulae, I separate the 256 different ECAs into distinct behavioral classes.",Allen Newell Award for Excellence in Undergraduate Research,Carnegie Mellon,Computer Science
2013,Reconstructing Dysarthic Speech from Cross-Speaker Articulatory Position Data using Speech Synthesis and Voice Conversion Techniques,"Dysarthria is a motor speech disorder that results from serious injury to a major component of the human speech system. Traditional speech synthesis techniques have often proven insufficient in constructing clear synthetic speech from source recordings of speakers affected by dysarthria. In this project, we propose an alternate approach to constructing a synthetic voice for a dysarthic speaker BF (a tongue cancer patient whose tongue had been surgically removed during cancer treatment) with the goal of constructing synthetic speech that both sounds clear and preserves distinctive acoustic features of BF's original voice. Our approach centers around the idea of constructing ""an artificial tongue"" for BF and using this along with information about the positions of BFâs other major articulators for any given sentence to build a voice. Since no information about the positions of BF's articulators is available, we use recordings and corresponding articulator position data (APD) of an individual who will be referred to as MSAK to construct an articulatory speech synthesizer that predicts APD given acoustics. Using the articulatory speech synthesizer and recordings of BF post-surgery, we determine the positions of all of BF's articulators except the tongue. Next, we synthesize recordings of MSAK speaking the same sentences as BF and run the articulatory speech synthesizer on newly synthesized MSAK recordings to determine the positions of MSAK's articulators for BF's sentences. We then use the predicted position of MSAK's tongue as an approximation of BF's tongue (the artificial tongue) and use this, along with the predicted positions of BF's other articulators to construct a voice for BF.",,Carnegie Mellon,Computer Science
2013,Unsupervised Arabic Word Segmentation and Statistical Machine Translation,"Word segmentation is a necessary step for Natural Language Processing (NLP) for morphologically rich languages, such as Arabic. In this thesis we experiment with unsupervised word segmentation systems proposed in the literature to perform word segmentation on Arabic, and then couple word segmentation with Statistical Machine Translation (SMT). Our results indicate that unsupervised segmentation systems turn out to be inaccurate and do not help with improving SMT quality. However, although minimal human post-processing greatly improves the translation accuracy, word baseline accuracy turns out to be better. We conclude that semi-supervised systems have more potential to improve Arabic to English translation in SMT.",,Carnegie Mellon,Computer Science
2013,Towards Computational Offloading in Mobile Device Clouds,"It is common practice for mobile devices to offload computationally heavy tasks off to a cloud, which has greater computational resources. In this work, our first contribution is to motivate the gain in computation time and energy consumption that can be made by offloading tasks to nearby devices inside a mobile device cloud (MDC). We do this by emulating network conditions that exist for different communication technologies provided by modern mobile devices. As opposed to emulated experiments, currently it is a challenge to carry out real experiments on mobile devices since no applications exist that can distribute different amounts of data and computation to nearby devices using different communication technologies. Thus, as a second contribution, we also present a platform that allows creation and offloading of tasks by a mobile devices to nearby devices. As a final contribution, we also present a test bed which we used to carry out experimentation and gain insights about what kind of strategies need to be employed in order to ensure losses in computation time and energy can be mitigated while performing task offload within an MDC.",,Carnegie Mellon,Computer Science
2013,An Authorization Model For The Web Programming Language Qwel,"With the fast growth of web technology, it is becoming easier for developers to design and deploy complex web applications. However, securing such web applications is becoming an increasing complex task as current technology provides limited support. Developers are required to reason about distributed computation and to write code using heterogeneous languages, often not originally designed with distributed computing in mind nor built-in security features. Qwel is an experimental type-safe functional programming language for the web that has dedicated primitives for publishing and invoking web services. In this paper, we propose to extend Qwel with a decentralized authorization model allowing service providers to secure the web applications written in Qwel. This extension will provide web developers with built-in primitives to issue credentials to users and to express access control policies. Therefore, when a protected web service is deployed, the security policy will be evaluated dynamically according to the credentials supplied by the user invoking this web service. As a result, we show how these new language features can be used to implement common scenarios as well as more sophisticated ones.",,Carnegie Mellon,Computer Science
2014,Certifying Compilations for Logic Programming,"We seek a certifying compiler for logic programs (in a core subset of Prolog), meaning a compiler whose output provably implements the source program. We present a dependently-typed virtual machine based on the Warren Abstract Machine (WAM), prove soundness and completeness of the VM, and implement a compiler that targets it. We then demonstrate how the type system is expressive enough to encode the correctness of the compiled program.",,Carnegie Mellon,Computer Science
2014,OpenISR 2.0,"I explore a new approach to ""Web-based Transient Personal Computing"" by building upon the ideas from Professor Satyanarayanan's ISR (Internet Suspend/Resume) to better fit modern VM technology and computing needs. ISR decouples PC state and hardware by storing the state in a virtual machine. It increases convenience and productivity for users by allowing them to securely access personal machine state on an anonymous machine (e.g. a PC in a waiting room at the doctorbarticulators to construct a voice for BF.s office) or more portable devices like tablets instead of being bound to particular hardware. Although the existing ISR is functional, its codebase is complex and some of the decisions made in its development would have been made differently today. I reconsider the original premises for ISR and build a system that can fill a niche today. Rather than working from the ISR codebase, I build upon the newer VMNetX codebase, which developed from ISR but differs in that its VMs are read-only and are stored and executed on-demand from a web server. The new implementation is novel approach to solving an otherwise well-established problem because of this HTTP-based storage and execution.",,Carnegie Mellon,Computer Science
2014,A Proof-Based Approach to Formalizing Protocols in Linear Epistemic Logic,"Linear epistemic logic can be used to reason about changing knowledge states of agents acting in a system. Here we use it to formalize the Needham-SchrC6der-Lowe public-key cryptographic protocol for establishing secure communication sessions. We have developed a notion of adequacy to refer to the formal compositional correspondence between the protocol and the formalism. Through the iterative process of attempting to prove adequacy theorems and noting where and how the proof breaks down, we have been able to refine the formalism so that it adheres to the structure and semantics of the protocol as it was originally specified. This work is the first step towards showing that rigorous formal reasoning can be applied to protocols and processes followed in the wild.",,Carnegie Mellon,Computer Science
2014,Compressing Natural Graphs and a Practical Work-Efficient Parallel Connectivity Algorithm,"Over the past two decades, the explosive growth of the internet has triggered an enormous increase in the size of natural graphs such as social networks and internet link-structures. Processing and representing large natural graphs efficiently in memory is thus crucial for a wide variety of applications. Our contributions are 1) A parallel graph processing framework for representing compressed graphs with significantly fewer bits per edge, and 2) A simple and practical expected linear-work, polylogarithmic depth parallel algorithm for graph connectivity. Natural graphs tend to contain a large amount of locality - vertices within a cluster mostly have edges to other vertices in their cluster. We discuss reordering techniques to make vertex labelings reflect locality inherent in the graph. We show that algorithms operating on compressed graphs in our framework are as fast, or faster than their uncompressed counterparts. We also test a variety of coding and relabling techniques, such as byte-codes, nibble codes and gap encoding. In the second part of the work, we describe a simple and practical work-efficient parallel algorithm for graph connectivity. The algorithm is based on recently developed techniques for generating low-diameter graph decompositions. We discuss implementing both the decomposition algorithm, and our connectivity algorithm in C++ using CILK+, and show that our connectivity algorithm achieves 9--19 times speedup relative to the best known sequential implementation of connectivity on 40 cores. The algorithm is also competitive with the fastest existing parallel connectivity implementations for large input graphs (0.88--1.41 times faster on a 40-core machine).",,Carnegie Mellon,Computer Science
2014,Compiler Correctness via Contextual Equivalence,"We have developed a methodology for verifying the correctness of the closure conversion phase of a compiler, adapted from the work by Ahmed. This lets us verify that individual components of programs are compiled correctly, so they can be linked with any other code and still behave as desired. We do this by using a shared language that encompasses both the source and target languages in which the compiled code can be reasoned about alongside its source. Our main improvement over previous methods is that we donbarticulators to construct a voice for BF.t need boundaries that separate the source and target language while inside the shared language.",,Carnegie Mellon,Computer Science
2014,Improved Methods in Semantic Relations,"The goal of this project was to develop improved methods of measuring semantic distance between strings. This was done using graphical methods over Wikipedia hyperlinks to generate features which were then combined to determine the semantic similarity between strings. The resulting scores were evaluated against a baseline in order to gauge performance improvements. The accuracy, speed, and scalability of the algorithm were the key issues targeted for improvement by this project.",,Carnegie Mellon,Computer Science
2014,Symbolic Summation in Difference Fields,"We seek to understand a general method for finding a closed form for a given sum that acts as its antidifference in the same way that an integral has an antiderivative. Once an antidifference is found, then given the limits of the sum, it suffices to evaluate the antidifference at the given limits. Several algorithms (by Karr and Schneider) exist to find antidifferences, but the papers describing these algorithms leave out several of the key proofs needed to implement the algorithms. We attempt to fill in these gaps and find that many of the steps to solve difference equations rely on being able to solve two problems: the equivalence problem and the homogenous group membership problem. Solving these two problems is essential to finding the polynomial degree bounds and denominator bounds for solutions of difference equations. We study Karr and Schneider's treatment of these problems and elaborate on the unproven parts of their work.",,Carnegie Mellon,Computer Science
2014,A Password Management Application with Provable Security and Minimal User Effort,"Passwords are used by millions of users everyday to protect their important online accounts. However, because of the increasing number of accounts and limited human memory, people tend to adopt unsafe practices including the usage of weak or identical passwords for various accounts. In order to solve this problem, we want to develop both secure and usable password management schemes, which are systematic strategies for a use to create and remember multiple passwords. Shared Cues, a password management scheme proposed by Blocki et al., makes use of natural rehearsals of passwords and mnemonic techniques to achieve provable strong security with minimal user effort. In their scheme, the user memorizes several vivid PersonActionObject stories, and uses subsets of the stories as passwords. The strategic sharing of PAO stories minimizes the number of stories that a user has to memorize, and also preserves security. Moreover, users can successfully maintain the memory of passwords by following a particular rehearsal schedule. The application which I am developing is an implementation of their scheme. Unlike other password managers, this application does not store any password or use any master password. It includes a memory game to facilitate with memorization of PAO stories, a reminder system to remind our users to rehearse stories that they have not practiced recently and a mechanism to help users recall stories if they forget. The web application is hosted on GitHub, and can also be used on mobile end utilizing cloud storage to allow synchronization across devices. For future work, we are planning to conduct user studies to gather feedback and advice on potential improvements to the application.",,Carnegie Mellon,Computer Science
2015,A Secure Human-Computable Authentication Scheme,"In a world where everyone is constantly watching, secure authentication is still possible. We propose a provably-secure authentication protocol that is both resistant to eavesdropping attacks, and can be carried out directly between you, the human user, and the authenticating computer, without any additional devices. The catch is that you must perform all the calculations in your head. However, we demonstrate that most humans are capable of this. We also give evidence that an adversary would have to spy on a significant number of authentications before being able to impersonate you. (Compared to the standard password scheme, where the adversary only has to see it once.) Specifically, we show that statistical adversaries must see 10^Omega(r) authentications in order to break the protocol, where r measures the length of the secret that the human user memorizes. Our work is based on recent results of Blocki et al. and Feldman et al. on statistical algorithm lower bounds which we apply to our protocol relying on the intractability of the k-junta problem.",,Carnegie Mellon,Computer Science
2015,Fast Approximation of Minimum 2-Hop Covers on Trees,"Distance queries on graphs is a fundamental problem for numerous real-world applications. Constant queries can be done with an APSP algorithm but that requires storing every pair of distances in Î©(n 2) space, whereas dynamically resolving distances requires O(m + n log(n)) time on m-edge n-vertex graphs. An algorithm that stores fewer precomputed values and reduces dynamic query time is the 2-hop labeling scheme where every vertex stores its distance (called a label) to a subset of the other vertices such that for any pair of vertices, they both have their distance to at least one vertex on their shortest path. The problem we study is minimizing the number of labels needed to correctly query distances. There is now a log(n) approximation algorithm for m-edge n-vertex graphs that runs in O(mn 3) time for this. We demonstrate this problem is NP-Complete, and we present the currently fastest algorithm which gives a log(n) approximation on n-vertex trees which runs in Î(nlog(n)). We also prove that this the optimal runtime for any correct tree-labeling algorithm.",,Carnegie Mellon,Computer Science
2015,Nipping Bugs in the Bud - Student Misconceptions in Introductory Computer Science Classes,"Bugs in software are often caused by misconceptions in the programmer's thought process. Such misconceptions ought to be 'nipped in the bud', in introductory computer science classes where there is a focus on underlying concepts. We aim to mitigate common misconceptions in the Principles of Imperative Computation course (15-122, taught in C0 and C) by analyzing the bugs committed by students in the class, determing the misconceptions behind these bugs, and devising ways to mitigate these misconceptions. We collected data about bugs committed by students throughout the course of the Fall 2014 semester (335 students) and during one lab in the Spring 2015 semester (288 students). We classified each bug according to a taxonomy based on the IEEE Standard Classification of Software Anomalies, determined the misconceptions behind commonly-seen bugs, and proposed teaching methods to eliminate them. The bugs observed were logic bugs (60 instances), data bugs (21), and interface bugs (7), as per the IEEE Standard Classification for Software Anomalies (2010). An additional category of bugs emerged called ""comprehension errors"" (32 instances) which are caused by a misunderstanding of concepts, specifications or error messages. In one of our weekly labs in Fall 2014, we noticed that 30% of the bugs were caused by lack of attention to edge cases, which are situations that occur at extremes that the programmer may not have considered. We performed an experiment during this lab in the Spring 2015 semester where we split the students in the class into two groups. We encouraged one group of students to spend five minutes thinking about edge cases in the problem before beginning to code, and did not give any special instructions to the second group. We expect that the students who think about edge cases prior to coding will perform better in terms of faster completion times, more incremental progress to solution, and better quality of final code.",,Carnegie Mellon,Computer Science
2015,Spliddit - Unleashing Fair Division Algorithms,"The field of fair division has been rapidly expanding in recent years, capturing the interest of researchers in economics, mathematics, and computer science. The Literature encompasses provably fair solutions for a wide variety of problems - many of them relevant to society at large. However, a few fair division systems are used in practice, with even fewer available to the public. Enter Spliddit, a first-of-its-kind website which provides easy access to carefully designed methods for dividing rent, goods, credit, chores, and fares. Since launching in November 2014, Spliddit has received coverage in popular technology websites such as Gizmodo and Fast Company, and has been used by tens of thousands of people. In this talk, we'll discuss the overall design and implementation of Spliddit, as well as the algorithmic details of Spliddit's various applications.",,Carnegie Mellon,Computer Science
2015,Cache Efficient Dynamic Programming Algorithms,"Parallel dynamic programing algorithms typically have undesirable I/O complexities. We studied solutions to two different problems which are representative of many problems with dynamic programming solutions. One is Local Alignment, which has a constant number of dependencies. We studied multiple algorithms and implemented some in C++ and Cilk+. We also analyzed the algorithms from a theoretical and practical standpoint, and then compared it to other dynamic programming algorithms for solving Local Alignment. The other is Optimal Binary Search Trees which contains problems with varying numbers of dependencies. For this problem, we designed and implemented a cache efficient dynamic programing algorithm using a divide and conquer technique. We also analyzed the algorithm theoretically and practically. Our algorithm has the same work complexity as current dynamic programming algorithms, but lower I/O complexity.",,Carnegie Mellon,Computer Science
2015,Quadratic Encoding for 3D Hand Pose Reconstruction,"Hand pose reconstruction has many uses in animation and virtual reality. However, because the movements are so fine and occlusion occurs frequently, it is difficult to accurately track hand motion. In cases where the full hand cannot be seen, it is often still possible to identify the fingertips. Our quadratic encoding method provides a way to reconstruct the full hand pose from fingertip position data to generate natural-looking hand motion in real time.",,Carnegie Mellon,Computer Science
2015,Data-Aware Auto-Tuning and Techniques for Improving Parallel Sorting Performance,"Sorting algorithms are essential in many contexts, and can make good use of parallelism, which modern machines make increasingly leverageable. We explore the use of auto-tuning to achieve high-performance parallel sorting. While auto-tuning is a technique that has been used in a variety of domains to automatically find parameters that optimize performance, most applications of auto-tuning involve algorithms that are data-indifferent. A particular sorting algorithm, by contrast, can vary quite a bit in performance depending on the distribution of the data being sorted. We develop an auto-tuner that allows us to easily optimize and evaluate a number of parallel sorting algorithms on many different architectures and datasets. Our auto-tuner allows us to explore how properties of the dataset affect the optimal tuning parameters, and we achieve performance improvement by letting these properties drive how parameters are selected. We also explore novel ways of using data-specific heuristics for further improving sorting performance. Our tuned sort outperforms the hand-tuned Problem-Based Benchmark Suite (PBBS) comparison sort (on which our code is based), achieves good performance on a wide variety of datasets, and parallelizes well over many cores.",,Carnegie Mellon,Computer Science
2015,Invertible Binary Transducers and the Automorphisms of the Binary Tree,An invertible binary transducer is a type of Mealy machine that encodes automorphisms of the infinite binary tree. The subgroup of Aut(2*) generated by these automorphisms is called a transduction group. Group theorists have become interested in transduction groups because they provide answers to classical questions such as Burnside's problem and constructing groups of intermediate growth. We focus on abelian transduction groups to answer fundamental decidability questions and prove structure theorems. We also explore automata theoretic questions regarding relations on 2* induced by elements of a transduction group and attempt to determine when they are regular.,,Carnegie Mellon,Computer Science
2015,An Autoencoder Triaging Algorithm for Acute Pancreatitis,"As more data is collected from clinical studies, predictive medicine will advance to the state where triaging can be automated using raw patient data. For acute pancreatitis, several scoring systems, such as Ranson and APACHE II, are employed clinically to measure the severity of acute pancreatitis; however, these criteria have limited accuracy. Designing more accurate scoring systems is difficult due to the small number of patients typically enrolled in studies and the small percentage of patients with acute pancreatitis whose condition becomes severe. In this thesis, we present scoring systems derived from machine learning classifiers that are trained on both raw patient data and clinical scores. We focus on measuring severity by predicting whether a patient will develop organ failure, the most common cause of death in patients with acute pancreatitis. Using stacked autoencoders, we attempt to learn deep feature representations of organ failure in acute pancreatitis, and we compare the results to those of shallow architectures. Ultimately, we found single autoencoder networks to perform with the best sensitivity (lowest number of false negatives). In addition to our results, we discuss our techniques for compensating for imbalanced data classes and missing patient data.",,Carnegie Mellon,Computer Science
2015,Active Sampling for Estimating Gaussian Graphical Models,"We consider the problem of learning the structure of high-dimensional Gauss Markov Random fields using L1-regularized linear regression. In many applications, such as in sensor networks and proteomics, it may be costly to obtain joint observations repeatedly from all the variables involved. To address this, we propose an active learning algorithm that directs the learning process by selectively focusing the sampling resources on more uncertain variables as it proceeds. We show theoretically that this results in significant savings in terms of the number of samples required per variable. Furthermore, we demonstrate experimental results that corroborate our theoretical findings.",,Carnegie Mellon,Computer Science
2015,Formal Verification of a Controlled Flight Between Two Robots: A Case Study,"Robots moving within controlled flight paths are complex systems that require formal verification of collision avoidance. A flight path dependent upon a coded program requires confidence of its ability to avoid crashing, which we show with a formal proof. Inspired by the controlled flight of two robots within the Disney-Pixar film WALLb""E, we have designed a controller for a robot flying within a complex controlled flight path, a helix with another robot. We have formally proven collision avoidance within the proof rules of differential dynamic logic, a logic for hybrid systems consisting of discrete controlled steps and continuous physics, using a deductive verification tool, KeYmaera, when this flight path is viewed in two dimensions as well as three dimensions. We formally prove safety as well as an additional property that the two robots are within some delta of each other. This case study also applies to aircraft collision avoidance and unmanned aerial vehicles where unsafe operation is potentially fatal and similar 3D motion is relevant.",,Carnegie Mellon,Computer Science
2015,Improved Methods for Optical Music Recognition,"Optical music recognition (OMR) is the problem of converting scanned music scores into a symbolic format such as MIDI. Basic information about the score is necessary for later steps, but is useful in itself to support many computer music applications. We developed new and improved methods to extract this information, which are used in a live score display application. Furthermore, we developed methods to estimate the quality of a scan, which can be used to select the highest-quality version of a score to use for OMR.",,Carnegie Mellon,Computer Science
2015,From Prediction to Decision Making in Intelligent Tutoring Systems,"Intelligent tutoring systems provide their students with an adaptive personalized learning experience. To do so, intelligent tutoring systems attempt to capture the state of their students through a student model. Student models have two primary uses: prediction of future student performance and instructional decision making. Since prediction performance is easier to quantify, student models are frequently judged by their predictive power. This has bred student models that are very powerful predictors, but cannot be easily used in decision making. In this work, we leverage these powerful predictors using novel decision algorithms that are compatible with almost any predictive student model. In particular we consider two decision problems: when to stop providing questions to the student and which skill to practice next. Our simulation results suggest that our when-to-stop decision algorithm acts similarly to existing decision algorithms with the added benefit of stopping when students are unable to progress given the current material. Our preliminary work on deciding between skills suggests that logistic regression models, previously only used for prediction, can be used to pick between skills and even learn a skill hierarchy.",,Carnegie Mellon,Computer Science
2015,Graph-Based Semi-Supervised Learning for Text Categorization through Supervised Random Walks,"Recently, many effective graph-based semi-supervised learning methods have been developed. For text categorization, a common method is label propagation through a bipartite graph of documents and features or a k-nearest neighbor graph of documents. In this talk, we consider a new approach based on supervised random walks.",,Carnegie Mellon,Computer Science
2015,Visual and Geometric Modeling of Lunar Surface Features,"In recent years, the discovery of skylights on the surface of the Moon and Mars has driven scientific interest, as they would make an ideal location for human settlement. This project develops the software and remote sensing capabilities necessary to generate a high resolution 3D mesh model of a skylight from observations of a landing spacecraft. Skylights are believed to be collapsed entrances to underground lava tubes, which could shelter a settlement from radiation, thermal extremes, and micrometeorites. At a much higher resolution then presently available orbital imagery, these techniques will allow scientists to study of the genesis of these skylights and determine the properties of possible caves underneath.",,Carnegie Mellon,Computer Science
2015,Staying Fair,"Fairness in the context of the allocation of goods is a universal construct whose violation elicits extremely strong reactions. It has nonetheless historically been mathematically ill-defined. In recent years, the situation has improved as economists, mathematicians, and computer scientists have tackled the issue of an axiomatic treatment of fairness. These axioms enable researchers to qualify algorithms for fair allocation and to meaningfully compare different mechanisms. None have yet extended this axiomatic approach to fairness over time: there is no axiomatic treatment of situations with multiple allocation events whose outcomes are linked beyond the naive iterated application of existing (ill-suited) axioms. I have extended the current core axioms for time invariant fair allocation to account for time. In particular, I have extended the axioms of strategyproofness, envy-freeness, and efficiency to be historically aware. Using the extended axioms, I have generalized the seminal probabilistic serial allocation mechanism of Bogomolnaia and Moulin to attain a strictly superior allocation mechanism in the canonical and iterated fair allocation settings. There may be better constructions of the axioms, and there are likely better mechanisms. My work lays the foundation for continued research in this space, and suggests that it is interesting, feasible, and worth pursuing.",,Carnegie Mellon,Computer Science
2015,"SIRTAS: State-of-the-art, Interactive, Real-time Twitter Sentiment Analysis System","Twitter has been the centre of social media analysis in recent years, especially when it comes to brand performance. However, the task of understanding public sentimentsb about a brand using their online engagement has been challenging and still unsolved. Through this research, I plan to continue my ongoing work in developing a system that can learn the sentiments of a tweet using Machine Learning. Further, this research would involve creating a Visualization Layer on top of our system that can enable nontechnical brand analysts to analyze processed tweets across the dimensions of time, location and consumer profile.",,Carnegie Mellon,Computer Science